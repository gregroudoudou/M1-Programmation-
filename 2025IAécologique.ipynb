{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXWqlBzw37Mg"
      },
      "source": [
        "I. Pourquoi ce projet est utile et quel va être le fil directeur ?\n",
        "\n",
        "Ce projet est né notamment dans le but de créer une IA environnementale. Grégoire avait eu l'idée, et l'avait même proposer en projet à M.Bianchini pour la conduite de projet.\n",
        "\n",
        "Ainsi, le but est de remplir un vide en matière d'IA, à savoir que peu conseillent sur les gestes écologiques du quotidien.\n",
        "\n",
        "De plus, nos  buts grâce à cette IA, est d'augmenter la consommation de produits locaux :\n",
        "-un : renforcer la consommation locale, et donc de circuit-court en résumant de façon efficace les informations : qui quoi, quoi  ;\n",
        "2. Permettre une comparaison facile et accessible avec le non-bio en termes économiques , en prenant comme points de comparaison un magasin non bio comme Leclerc\n",
        "3. Renforcer les achats de produits locaux/bios parmi les populations plus populaires, en tentant de calculer les coûts liés à chaque panier s'ils ne sont pas présent ;\n",
        "4. Tenter de faire une mesure économique au long-terme, en termes de santé pour la personne, et en termes d'économie de coûts.\n",
        "5. Permettre de le rendre directement utilisable le plus possible.\n",
        "\n",
        "A noter, il est important de souligner qu'utiliser l'IA pour l'écologie est également un peu paradoxal : une requête d'IA utilise en moyenne dix fois plus d'énergie qu'une requête Google. Ainsi, il est important pour nous que cette IA puisse plus conseiller, et améliorer la prise de décision, afin que son usage soit intéressant en comparaison d'une recherche Google."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWhSlTIwvMjJ"
      },
      "source": [
        "I. Création de la base de données\n",
        ":THOMAS\n",
        "+CODE :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSF1cP0rvL6K"
      },
      "source": [
        "A noté,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHtdPeUu8Sg1"
      },
      "source": [
        "II. Un pélerinage en IA :Essais fructueux avec  Mistral ,moins fructueux lors de l'entraînement sur des données spécifiques. :\n",
        "![image.png](data:image/png;base64,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)![image.png](\n",
        "  \n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9NbWG3ptvQv"
      },
      "source": [
        "Mistral avait été le premier choix. En effet, il était présenté comme plus léger que les autres modèles tout en étant français.\n",
        "\n",
        "\n",
        "En étant donc assez optimiste, on a testé...et cela marchait plutôt bien pour des questions générales, ne nécessitant pas de l'entraîner. Il tournait plutôt bien sur colab.\n",
        "\n",
        "Or entraîner un modèle, c'est aussi mobilisé plus de RAM du GPU.  Entraîner un modèle d'IA signifie notamment qu'on va l'entraîner à être plus précis sur une base de donnée définie.\n",
        "De plus, on avait fait attention à diminuer au maximum la RAM, avec du Low rank, càdire l'entraînement d'une partie des hyperparamètres à la place de la totalité, et l'utilisation d'un modèle avec une quantification de 4 bits.\n",
        "\n",
        "Toutefois, de nombreuses erreurs GPU sont apparues, en raison d'une quantité de RAM trop importante qui a été utilisée. Ainsi, il n'y avait aucun résultat qui s'affichait, et Google Colab nous déconnectait régulièrement pour trop utiliser leurs serveurs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Jnr-fguG2I"
      },
      "source": [
        "## II.La recherche d'alternatives : Llama, OPT, et Mistral 7b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfwK2RvZwfFP"
      },
      "source": [
        "# **Partie 1 : pré-requis **\n",
        "\n",
        "\n",
        "# I.Pré-requis matériel et choix du matériel : quels impacts écologiques ?\n",
        "\n",
        "De facto, il apparaît nécessaire de connecter ce code **au GPU T4**, de Collab afin que cela marche.\n",
        "\n",
        "**1.Cliquez à côté de l'endroit de la RAM sur Collab **\n",
        "**2. Modifier le type d'exécution **\n",
        "**3. Cocher GPU T4**\n",
        "**Sinon ce script ne peut fonctionner car on a besoin d'un GPU à part afin d'effectuer les calculs du LLM**.\n",
        "\n",
        "Quand on développe une intelligence artificielle qui conseille des gens sur des produits disponibles près de chez eux, on peut vouloir utiliser un modèle de langage comme LLaMA pour bien comprendre les besoins des utilisateurs et générer des réponses naturelles.\n",
        "\n",
        "Mais ces modèles sont **très gros**. Par exemple, LLaMA 7B a 7 milliards de paramètres. Ça demande pas mal de ressources.\n",
        "\n",
        "\n",
        "Les modèles comme LLaMA sont très puissants, mais aussi très gros. Même la version “petite”, LLaMA 7B, a 7 milliards de paramètres. Ça demande beaucoup de mémoire et de calculs pour fonctionner. Impossible de faire tourner ça sur un ordi classique.\n",
        "\n",
        "C’est là qu’un GPU comme le NVIDIA T4 est super utile.\n",
        "\n",
        "Le **T4 a 16 Go de mémoire**, ce qui est suffisant pour faire tourner une version compressée (quantisée) du modèle, par exemple LLaMA 7B en 4-bit. Il n’a pas besoin d’être aussi gros ou cher qu’un A100 ou H100.\n",
        "\n",
        "**2. Un GPU T4 plus écologique**\n",
        "Il utilise peu d’énergie (70W) contre **400 watts pour un A100**, ce qui le rend parfait pour des déploiements simples, en cloud ou en local.\n",
        "\n",
        "Il est aussi optimisé pour les calculs IA grâce à ses Tensor Cores, qui accélèrent les opérations sur les matrices, très utilisées par les modèles de langage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6svppYzv7W8W"
      },
      "source": [
        "# 2.**Pré-requis de programmation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_67j-rI38aIf"
      },
      "source": [
        "I. Installer les packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6f0ixIL9nPU",
        "outputId": "73b358d9-bb08-4a97-d611-b1af2fda4db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.13)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.0/118.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers  # Remove version restriction here\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets\n",
        "!pip install evaluate\n",
        "!pip install -qqq trl==0.7.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1QyKCqlE2jf"
      },
      "source": [
        "2. Charger les packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6_5n9zv7-537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "outputId": "37b309b6-b760-4925-c4e6-40c4e17c42df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error importing huggingface_hub.hf_api: cannot import name 'XetAuthorizationError' from 'huggingface_hub.errors' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/errors.py)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'XetAuthorizationError' from 'huggingface_hub.errors' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/errors.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7043c3c1055f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluation_suite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluationSuite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m from .evaluator import (\n\u001b[1;32m     31\u001b[0m     \u001b[0mAudioClassificationEvaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/evaluation_suite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"3.5.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murl_to_fs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m from huggingface_hub import (\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mCommitInfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mCommitOperationAdd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{package_name}.{name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m from ._commit_api import (\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mCommitOperation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mCommitOperationAdd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/_commit_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXetAuthorizationError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXetRefreshTokenError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfile_download\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhf_hub_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlfs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUploadInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlfs_upload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_lfs_batch_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'XetAuthorizationError' from 'huggingface_hub.errors' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/errors.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset, load_dataset\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJcrg3g4mvwR"
      },
      "source": [
        "# 3. Importer directement depuis Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW5udh9uKWVS"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# ID du fichier Google Drive\n",
        "file_id = \"1xl9bPoZ715I6dkiJz7NUtghms--eDmCy\"\n",
        "# URL de téléchargement directe (via Google Drive)\n",
        "url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "# Télécharger le fichier\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()  # en cas d'erreur\n",
        "\n",
        "# Charger le contenu JSON\n",
        "data = json.loads(response.text)\n",
        "\n",
        "# Convertir en DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Afficher les premières lignes\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yB7KPbh7lEe"
      },
      "source": [
        "# **B. Intérêts d'un modèle open-source afin de pouvoir diminuer les biais ou une orientation trop conservatrice/trop progressive(hallucination) du modèle. **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQNHKhmP-h6E"
      },
      "source": [
        "I. Choix du modèle: dans quelle mesure est-il important d'utiliser **un LLM** qu'on entraîne nous même et non une API ?\n",
        "\n",
        "La réponse est simple : la possibilité de paramétrer, afin d'influencer comme on le souhaite la réponse donnée. D'un autre côté, cela permet d'éviter **d'être dépendant des paramétrages initiaux**, qui sont parfois fortement orientées, même politiquement, comme ** ceux de Grok**( sauf rébellion).\n",
        "\n",
        "**Meta (Facebook)** a créé **OPT**, une série de modèles de langage (transformers) allant de 125 millions à 175 milliards de paramètres.\n",
        "Ils sont conçus pour ressembler à **GPT-3**, en termes de taille et de performance.\n",
        "Il est sorti en **2022**.\n",
        "\n",
        "**Pourquoi cette appellation OPT ?**\n",
        "En effet, cette appellation existe en raison du caractère **open-source** de l'OPT.\n",
        "\n",
        "Nous présentons **Open Pretrained Transformers (OPT)**, une suite de transformateurs pré-entraînés, uniquement pour décodeur, avec des paramètres compris **entre 125 M et 175 B**, que nous souhaitons partager pleinement et de manière responsable avec les chercheurs intéressés\n",
        "\n",
        "**Quel niveau de performance par rapport aux IA que tout le monde connaît ?**\n",
        "Les performances des modèles **OPT**correspondent approximativement aux performances et aux tailles des modèles de la classe GPT-3 comme **Chat gpt3.5**, tout en appliquant les meilleures pratiques les plus récentes en matière de collecte de données et d'apprentissage efficace\n",
        "\n",
        "**Quel est le but **social** et dans quelle mesure ce modèle a -t-il vocation a encouragé **le progrès social ** ? **\n",
        "Cet accès restreint a limité la capacité des chercheurs à étudier le fonctionnement de ces grands modèles linguistiques, freinant ainsi les progrès visant à résoudre les problèmes connus tels que **la robustesse, les biais et la toxicité**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**En résumé : pourquoi entraîner nous même un LLM ?**\n",
        "\n",
        " Ainsi, le choix **d'un modèle LLM à part, et non pré-configurer** nous apparaît intéressant de souligner l'envie de créer ** des modèles open-sources**, dans le but à ce qu'il existe ** une diversité de paramétrages**, comme la température qui permet  de **modifier la créativité du modèle**.\n",
        " Ces éléments sont la clé afin de créer des modes de **pensée différents**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCgADxTJBmFp"
      },
      "source": [
        "2.**¨Pourquoi un modèle Facebook de 125M marche-t-il fonctionne-t-il alors que le Mistral fonctionnait un peu et le Llama (2023), version plus évoluée, ne fonctionnait pas ? **\n",
        "\n",
        "1.OPT-125M est minuscule, **il est beaucoup plus petit que les 7 milliards de paramètres d'un Llama par exemple **.Ainsi, il est au final, pour donner un ordre de grandeur **50 fois plus petit**.\n",
        "En conséquence, c'est beaucoup plus simple à entraîner !\n",
        "\n",
        "2.**Le deuxième avantage est qu'on n'a pas besoinde **quantisé**.\n",
        "**Quantisé signifie qu'on transforme un langage complexe de 16bits par exemple 8.0120304958 à du langage plus simpel comme du 4 bits, 8.0. Ensuite, on déquantifie la sortie pour tenter de la retraduire en 16 bits, qui est le langage inital.\n",
        "\n",
        "En effet, lorsque j'ai fait tourner **Mistral**, on a réussit à le faire tourner mais pas à l'**entraîner**\n",
        "\n",
        "Il ne fait que 125 millions de paramètres, c’est presque rien pour un LLM.En conséquence, **la quantification/simplification** du langage est **énorme**.\n",
        "\n",
        "**Quels limites de cette méthode et de cette très petite taille de paramètre , que l'on peut donc constater **\n",
        "\n",
        "Un modèle comme OPT-125M :\n",
        "\n",
        "A une toute** petite \"mémoire\" du langage**.\n",
        "\n",
        "Ne comprend pas bien** les contextes longs**.\n",
        "\n",
        "Génère souvent des **réponses simplistes ou imprécises.**\n",
        "\n",
        "**Toutefois, ces performances semblaient assez honorables, voire plus que Bert**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3RItRsfBlsH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRJ85J08Ei_X"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc1jY7CCOovK"
      },
      "source": [
        "# C. LLama , un modèle trop lourd en paramètres pour tourner sous Colab\n",
        "\n",
        "Sous LLAMA, en structurant mieux le dataset , on réussit également à mieux finetuner le modèle, d'où cette petite partie qui permet égalemetn d'améliroer au mieux notre modèle.\n",
        "\n",
        "\n",
        "Ce qui est demandé pour qu'un Json dans Meta IA (Facebook Opt) soit** optimal**, et que l'entraînement soit donc plus efficace est de créer une sorte de scission entre demande/instruction.\n",
        "-demande\n",
        "-instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLHH6lJHK_hc"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use the existing df DataFrame instead of trying to access an undefined dataset\n",
        "# df = dataset.to_pandas()  # Remove this line\n",
        "\n",
        "# Split en train+val / test\n",
        "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Split train+val → train / val\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convertir chaque DataFrame en Dataset\n",
        "dataset = DatasetDict({  # Now 'dataset' is being defined\n",
        "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
        "    \"validation\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
        "    \"test\": Dataset.from_pandas(test_df.reset_index(drop=True))\n",
        "})\n",
        "\n",
        "# Afficher la structure\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMftK-gL-Jpu"
      },
      "outputs": [],
      "source": [
        "def format_instruction(article: str, summary: str):\n",
        "\treturn f\"\"\"### Instruction:\n",
        "Summarize the following conversation.\n",
        "\n",
        "### Input:\n",
        "{article.strip()}\n",
        "\n",
        "### Summary:\n",
        "{summary}\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKCBgMBU-TUZ"
      },
      "outputs": [],
      "source": [
        "def generate_instruction_dataset(data_point):\n",
        "\n",
        "    return {\n",
        "        \"url\": data_point[\"url\"],\n",
        "        \"texte\": data_point[\"texte\"],\n",
        "        \"texte\": format_instruction(data_point[\"texte\"],data_point[\"texte\"])\n",
        "    }\n",
        "    ## Remplacé datapoint['article'] par datapoint['texte'] et data_point['highlights'] par data_point['texte']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a20yfy61N7l0"
      },
      "source": [
        "\n",
        "Elle sert à reformater une donnée brute  en un format structuré, probablement pour l'entraîner dans un modèle de type LLM / Instruction-tuned, comme un modèle de génération de texte (ex. T5, GPT, Qwen, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TO2M1-iS-WlN"
      },
      "outputs": [],
      "source": [
        "def process_dataset(data: Dataset):\n",
        "    return (\n",
        "        data.shuffle(seed=42)\n",
        "        .map(generate_instruction_dataset)\n",
        "        # Removing remove_columns(['id']) as it is no longer needed\n",
        "        #.remove_columns(['id'])\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrDqMZkqOGij"
      },
      "source": [
        "La fonction process_dataset applique un mélange aléatoire, reformate chaque exemple avec generate_instruction_dataset que j'ai créé juste avant , puis supprime la colonne id du dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQUXFx71-YbX"
      },
      "outputs": [],
      "source": [
        "## APPLYING PREPROCESSING ON WHOLE DATASET\n",
        "dataset[\"train\"] = process_dataset(dataset[\"train\"])\n",
        "# Using dataset[\"validation\"]  instead of test to access original data\n",
        "dataset[\"test\"] = process_dataset(dataset[\"validation\"])\n",
        "# Removing as it is already applied\n",
        "#dataset[\"validation\"] = process_dataset(dataset[\"validation\"])\n",
        "# Get the size of the training dataset\n",
        "train_size = len(dataset['train'])\n",
        "\n",
        "# Select a maximum of 500 rows (or the entire dataset if smaller)\n",
        "train_data = dataset['train'].shuffle(seed=42).select([i for i in range(min(train_size, 3000))])\n",
        "\n",
        "# Get the size of the test and validation datasets\n",
        "test_size = len(dataset['test'])\n",
        "validation_size = len(dataset['validation'])\n",
        "\n",
        "# Select a maximum of 50 rows (or the entire dataset if smaller)\n",
        "test_data = dataset['test'].shuffle(seed=42).select([i for i in range(min(test_size, 50))])\n",
        "validation_data = dataset['validation'].shuffle(seed=42).select([i for i in range(min(validation_size, 50))])\n",
        "\n",
        "train_data,test_data,validation_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaStDV1M8EcO"
      },
      "outputs": [],
      "source": [
        "train_data[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsutZv-iD8_a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RilkLYvlD7L_"
      },
      "source": [
        "# ** D.Test du Llama  **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6_7D9PGJn9u"
      },
      "source": [
        "1.Test avec une question générale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le40g8qXAlSS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Essayer une approche différente pour installer les paquets tout en évitant les conflits\n",
        "!pip install --upgrade pip\n",
        "!pip install numpy torch --no-deps\n",
        "!pip install -q bitsandbytes --no-deps\n",
        "!pip install transformers --no-deps\n",
        "!pip install accelerate --no-deps\n",
        "\n",
        "# Maintenant nous chargeons les bibliothèques nécessaires\n",
        "import torch\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Configurer la quantification 4 bits\n",
        "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "# Utilisez les blocs try-except pour gérer les erreurs potentielles\n",
        "try:\n",
        "  # Charger le modèle avec quantification\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        use_auth_token=False,  # Avoid authentication issues\n",
        "        trust_remote_code=True  # Allow loading remote code if needed\n",
        "    )\n",
        "\n",
        "   # Charger le tokeniseur\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    print(\"Model and tokenizer loaded successfully!\")\n",
        "\n",
        "# Fonction pour générer une réponse textuelle\n",
        "    def generate_response(prompt, max_length=256):\n",
        "        try:\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_length,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.4,#Une faible température permet de diminuer la créativité.\n",
        "                    top_p=0.9,\n",
        "                )\n",
        "\n",
        "            return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        except Exception as e:\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "    # Exemple d'utilisation avec un prompt français\n",
        "    prompt = \"Trouve moi une solution pour améliorer l'efficacité énergétique.\"\n",
        "    print(\"\\nPrompt:\", prompt)\n",
        "    print(\"\\nResponse:\", generate_response(prompt))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {str(e)}\")\n",
        "    print(\"\\nTrying alternative approach...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc15pSboxxzf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMM7M7ZpFcr9"
      },
      "source": [
        "# Partie 3:Facebook OPT, un conseiller qui doit garder les pieds sur terre!\n",
        "\n",
        "En effet, comme vu auparavant,le faible nombre de paramètres que l'on a peut donc conduire à de fortes **\"hallucinations\"** du modèle.Il semble être à côté de la plaque.\n",
        "\n",
        "Ainsi, on est face à un **dilemne cornélien** : faut-il **avoir un modèle plus complexe , mais avec le grand défaut d'utiliser trop de vRAM, et donc de tout faire bugger ? **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fPk4N7kFhwX"
      },
      "outputs": [],
      "source": [
        "\n",
        "    # On tente un modèle plus petit et avec une réponse longue.\n",
        "    try:\n",
        "        alternative_model_id = \"facebook/opt-125m\"  # Much smaller model\n",
        "        model = AutoModelForCausalLM.from_pretrained(alternative_model_id, device_map=\"auto\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(alternative_model_id)\n",
        "\n",
        "        def generate_response(prompt, max_length=1000):\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            with torch.no_grad():\n",
        "                output = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_length,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.7,\n",
        "                )\n",
        "            return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        print(\"Modèle léger\")\n",
        "        prompt = \"Propose des solutions pour l'environnement ? \"\n",
        "        print(\"\\nPrompt:\", prompt)\n",
        "        print(\"\\nRéponse:\", generate_response(prompt))\n",
        "    except Exception as e:\n",
        "        print(f\"Approche alternative: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iywoYEg-FrCs"
      },
      "source": [
        "# **2.La méthode **\"Luna Lovegood\"**\n",
        "\n",
        "Cette méthode que j'ai inventée, a le but de diminuer le nombre **d'hallucinations**.\n",
        "1.**Quantification**celle notamment de tester un nombre d'**hyperparamétrages** du plus petit au plus grand, afin de voir les améliorations du modèle\n",
        "2.**Traduction du prompt** : les prompts **en anglais** sont plus performants et **hallucinent** moins.\n",
        "\n",
        "\n",
        "**Peux-t-on simplifier le langage tout en gardant des performances similaires (littérature scientifiques)**?\n",
        "\n",
        "Ainsi, on va passer à un modèle à 1,3 bits et tenter d'analyser s'il **hallucine** pas trop\n",
        "\n",
        "\n",
        "Par ailleurs, en plus de jouer sur le nombre d'hyperparamètres, on peut **en plus abaisser la quantification**dans le but **que notre modèle transmet ses informations dans un format numérique simplifié**.\n",
        "\n",
        "Selon les études Evaluating Quantized Large Language Models Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang. Cette étude évalue l'impact de la quantification post-entraînement (PTQ) sur 11 familles de modèles, dont OPT, LLaMA2, Falcon, Bloomz, Mistral, etc., couvrant des tailles de 125M à 180B paramètres. Les résultats montrent que **la quantification peut réduire significativement la mémoire et le coût computationnel** tout en maintenant des performances **comparables** aux modèles **non quantifiés** sur diverses tâches.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgxfI39PKU7N"
      },
      "source": [
        "# **Modèle 1:une hallucination forte, sensible à la traduction anglais/français**\n",
        " Le 1,3 Bits est toujours dans un \"délire\" de réponse \"énergique\".\n",
        "\n",
        "Cela est un bel  exemple d'hallucination d'une IA ! Et on va augmenter le nombre d'hyperparamètres pour résoudre ce problème, et voir quand on sera satisfait !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-0cbUARGev8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# On tente un modèle un peu plus gros (1.3 milliards de paramètres)\n",
        "try:\n",
        "    alternative_model_id = \"facebook/opt-1.3b\"  # Modèle plus grand\n",
        "    model = AutoModelForCausalLM.from_pretrained(alternative_model_id, device_map=\"auto\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(alternative_model_id)\n",
        "\n",
        "    def generate_response(prompt, max_length=1000):\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_length,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "            )\n",
        "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"Modèle OPT 1.3B\")\n",
        "    prompt = \"Suggest solutions for the environment? \"\n",
        "    #Propose des solutions pour l'environnement?: prompt à changer  suivant qu'on veut voir si les réponses sont modifiées entre anglais/français.\n",
        "    print(\"\\nPrompt:\", prompt)\n",
        "    print(\"\\nRéponse:\", generate_response(prompt))\n",
        "except Exception as e:\n",
        "    print(f\"Erreur avec le modèle OPT-1.3B : {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ainsi, on peut constater que le résultat est **peu fiable**. On a l'impression que notre **IA** a des **hallucinations**  mais uniquement **en français**.\n",
        "En effet **en anglais**, il indique a contrario **qu'un \"rien\"** , comme du sel, peut avoir un impact écologique important .\n",
        "On peut souligner **que l'anglais** semble donner des résultats plus **fiables **."
      ],
      "metadata": {
        "id": "CXA8BjNDADbo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQbdJdJHKbVj"
      },
      "source": [
        "# **2.Test avec augmentation croissante du nombre d'hyperparamètres des modèles : **"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── 1. Installer / compiler AutoGPTQ avec support CUDA ───────────────────────\n",
        "# Mets à jour pip, clone et installe AutoGPTQ depuis le repo pour activer les kernels CUDA\n",
        "!pip install -q --upgrade pip\n",
        "!git clone https://github.com/PanQiWei/AutoGPTQ.git\n",
        "!pip install auto-gptq\n",
        "%cd AutoGPTQ\n",
        "# Active la compilation des extensions CUDA\n",
        "!BUILD_CUDA_EXT=1 pip install -v .\n",
        "%cd ..\n",
        "\n",
        "# Installer le reste\n",
        "!pip install -q transformers accelerate optimum triton\n",
        "\n",
        "# ─── 2. Imports ────────────────────────────────────────────────────────────────\n",
        "import torch, time\n",
        "from transformers import AutoTokenizer, TextStreamer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "\n",
        "# ─── 3. Prompt de test ─────────────────────────────────────────────────────────\n",
        "prompt = \"Propose des solutions concrètes pour préserver l'environnement.\"\n",
        "\n",
        "# ─── 4. Modèles publics (TheBloke) par quantification ─────────────────────────\n",
        "models_by_bits = {\n",
        "    \"2-bit /3bits\": [\n",
        "        \"TheBloke/deepseek-coder-6.7b-instruct.Q2_K.gguf\", #codé réellement en 2 bits\n",
        "        #Max RAM : 5GO\n",
        "        \"TheBloke/deepseek-coder-6.7b-instruct.Q3_K_S.gguf\" #pré quantifié en 3 bits.\n",
        "\n",
        "    ],\n",
        "    \"4-bit\": [#Moins de pertes et techniques d'optimisations utiles !\n",
        "        \"TheBloke/Llama-2-7B-Chat-GPTQ\",\n",
        "        \"TheBloke/zephyr-7B-alpha-GPTQ\",\n",
        "        \"TheBloke/Mistral-7B-v0.1-GPTQ\",\n",
        "        \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\",\n",
        "        \"TheBloke/gptq-4bit-32g-actorder_True\",\n",
        "        \"TheBloke/deepseek-coder-6.7b-instruct.Q4_K_S.gguf\",#un deeseek oeu de perte  !\n",
        "        \"openchat_3.5.Q4_K_M.gguf\"#un chat gpt à corriger:meilleur compromis qualité/réussite\n",
        "        ##moins de perte de qualité pour ce modèle ! :\n",
        "    ],\n",
        "    \"5-bit\": [\n",
        "        \"TheBloke/CodeLlama-7B-Instruct-GPTQ\"\n",
        "\n",
        "    ],\n",
        "    \"6-bit\": [\n",
        "        \"TheBloke/falcon-7b-instruct-GPTQ\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# ─── 5. Fonction de génération ─────────────────────────────────────────────────\n",
        "def generate_from_model(model_id):\n",
        "    print(f\"\\n🔍 Chargement : {model_id}\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "        model = AutoGPTQForCausalLM.from_quantized(\n",
        "            model_id,\n",
        "            device_map=\"auto\",           # répartit sur CUDA automatiquement\n",
        "            use_safetensors=True,\n",
        "            trust_remote_code=True,\n",
        "            use_triton=True               # active les kernels triton\n",
        "        )\n",
        "\n",
        "        # On récupère une device valide\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        streamer = TextStreamer(tokenizer)\n",
        "\n",
        "        print(\"✍️ Génération en cours…\")\n",
        "        t0 = time.time()\n",
        "        _ = model.generate(\n",
        "            **inputs,\n",
        "            streamer=streamer,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "        print(f\"\\n✅ Fini en {time.time()-t0:.2f}s\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Échec {model_id} : {e}\")\n",
        "\n",
        "# ─── 6. Lancer les tests ───────────────────────────────────────────────────────\n",
        "for bits, mids in models_by_bits.items():\n",
        "    print(f\"\\n\\n🧠 === Tests en {bits} ===\")\n",
        "    for mid in mids:\n",
        "        generate_from_model(mid)\n"
      ],
      "metadata": {
        "id": "c3sFn7FUYVo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ─── 1. Installer / compiler AutoGPTQ avec support CUDA ───────────────────────\n",
        "# Mets à jour pip, clone et installe AutoGPTQ depuis le repo pour activer les kernels CUDA\n",
        "!pip install -q --upgrade pip\n",
        "!git clone https://github.com/PanQiWei/AutoGPTQ.git\n",
        "%cd AutoGPTQ\n",
        "# Active la compilation des extensions CUDA\n",
        "!BUILD_CUDA_EXT=1 pip install -v .\n",
        "%cd ..\n",
        "\n",
        "# Installer le reste\n",
        "!pip install -q transformers accelerate optimum triton"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Yn1vPoQ0uY2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ainsi, on peut remarquer **plusieurs éléments**\n",
        "**1. Certains modèles ne sont pas **accessibles** car ils sont dans des **respository privé** qui font payé **au mot /token** près.\n",
        "\n",
        "**2.**Les modèles ayant de fortes **bits** semblent ne pas fonctionné , étant donné que charger le modèle pose un **problème** à partir d'une quantification de **3 bits**.\n",
        "\n",
        "**3.**Mistral**répond automatiquement en anglais **"
      ],
      "metadata": {
        "id": "doC3yUbUA2oK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Correction des erreurs précédentes : l'utilisation d'autres packages pour Deepseek"
      ],
      "metadata": {
        "id": "9cyHF0-H197f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import subprocess, sys, textwrap, json, os\n",
        "from multiprocessing import Process, Queue\n",
        "from pathlib import Path\n",
        "\n",
        "REPO   = \"TheBloke/deepseek-coder-6.7B-instruct-GGUF\"\n",
        "FILES  = [\n",
        "    \"deepseek-coder-6.7b-instruct.Q2_K.gguf\",\n",
        "    \"deepseek-coder-6.7b-instruct.Q3_K_S.gguf\",\n",
        "    \"deepseek-coder-6.7b-instruct.Q3_K_M.gguf\",\n",
        "]\n",
        "LOCAL  = Path(\"models\")\n",
        "\n",
        "PROMPTS = [\n",
        "    \"### Instruction:\\nExplique la différence entre une liste et un dictionnaire en Python.\\n### Response:\\n\",\n",
        "    \"### Instruction:\\nÉcris une fonction Python qui retourne la somme d'une liste.\\n### Response:\\n\"\n",
        "]\n",
        "\n",
        "###############################################################################\n",
        "def worker(repo, gguf, prompts, q):\n",
        "    \"\"\"Charge 1 modèle, exécute les prompts, renvoie les infos puis meurt.\"\"\"\n",
        "    import time, torch, gc\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "    #   1) download (si absent) ------------------------------------------------\n",
        "    LOCAL.mkdir(exist_ok=True)\n",
        "    path = LOCAL / gguf\n",
        "    if not path.exists():\n",
        "        path = hf_hub_download(repo, gguf, local_dir=str(LOCAL))\n",
        "\n",
        "    #   2) auto GPU layers -----------------------------------------------------\n",
        "    gpu_layers = 0\n",
        "    if torch.cuda.is_available():\n",
        "        vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "        gpu_layers = int(max(0, min(32, (vram-2)/0.25)))\n",
        "\n",
        "    #   3) load & run ----------------------------------------------------------\n",
        "    llm = AutoModelForCausalLM.from_pretrained(str(path), model_type=\"deepseek\",\n",
        "                                              gpu_layers=gpu_layers)\n",
        "    out = []\n",
        "    for p in prompts:\n",
        "        t0 = time.perf_counter()\n",
        "        txt = llm(p, max_new_tokens=128, temperature=0.7)\n",
        "        dt  = time.perf_counter()-t0\n",
        "        out.append(dict(tok=len(txt.split()), sec=dt, answer=txt[:300]))\n",
        "    #   4) return then free ----------------------------------------------------\n",
        "    q.put((gguf, gpu_layers, out))\n",
        "    del llm ; gc.collect()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "###############################################################################\n",
        "def run_all():\n",
        "    LOCAL.mkdir(exist_ok=True)\n",
        "    for gguf in FILES:\n",
        "        q = Queue()\n",
        "        p = Process(target=worker, args=(REPO, gguf, PROMPTS, q))\n",
        "        p.start()\n",
        "        p.join()         # attend la fin ➜ RAM rendue\n",
        "        if not q.empty():\n",
        "            model, layers, outs = q.get()\n",
        "            speed = sum(o[\"tok\"]/o[\"sec\"] for o in outs)/len(outs)\n",
        "            print(f\"✅ {model}  |  GPU layers={layers}  |  {speed:.2f} tok/s\")\n",
        "        else:\n",
        "            print(f\"❌ {gguf} a échoué\")\n",
        "#!/usr/bin/env python3\n",
        "import subprocess, sys, textwrap, json, os\n",
        "from multiprocessing import Process, Queue\n",
        "from pathlib import Path\n",
        "\n",
        "REPO   = \"TheBloke/deepseek-coder-6.7B-instruct-GGUF\"\n",
        "FILES  = [\n",
        "    \"deepseek-coder-6.7b-instruct.Q2_K.gguf\",\n",
        "    \"deepseek-coder-6.7b-instruct.Q3_K_S.gguf\",\n",
        "    \"deepseek-coder-6.7b-instruct.Q3_K_M.gguf\",\n",
        "]\n",
        "LOCAL  = Path(\"models\")\n",
        "\n",
        "PROMPTS = [\n",
        "    \"### Instruction:\\nExplique la différence entre une liste et un dictionnaire en Python.\\n### Response:\\n\",\n",
        "    \"### Instruction:\\nÉcris une fonction Python qui retourne la somme d'une liste.\\n### Response:\\n\"\n",
        "]\n",
        "\n",
        "###############################################################################\n",
        "def worker(repo, gguf, prompts, q):\n",
        "    \"\"\"Charge 1 modèle, exécute les prompts, renvoie les infos puis meurt.\"\"\"\n",
        "    import time, torch, gc\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "    #   1) download (si absent) ------------------------------------------------\n",
        "    LOCAL.mkdir(exist_ok=True)\n",
        "    path = LOCAL / gguf\n",
        "    if not path.exists():\n",
        "        path = hf_hub_download(repo, gguf, local_dir=str(LOCAL))\n",
        "\n",
        "    #   2) auto GPU layers -----------------------------------------------------\n",
        "    gpu_layers = 0\n",
        "    if torch.cuda.is_available():\n",
        "        vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "        gpu_layers = int(max(0, min(32, (vram-2)/0.25)))\n",
        "\n",
        "    #   3) load & run ----------------------------------------------------------\n",
        "    llm = AutoModelForCausalLM.from_pretrained(str(path), model_type=\"deepseek\",\n",
        "                                              gpu_layers=gpu_layers)\n",
        "    out = []\n",
        "    for p in prompts:\n",
        "        t0 = time.perf_counter()\n",
        "        txt = llm(p, max_new_tokens=128, temperature=0.7)\n",
        "        dt  = time.perf_counter()-t0\n",
        "        out.append(dict(tok=len(txt.split()), sec=dt, answer=txt[:300]))\n",
        "    #   4) return then free ----------------------------------------------------\n",
        "    q.put((gguf, gpu_layers, out))\n",
        "    del llm ; gc.collect()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "###############################################################################\n",
        "def run_all():\n",
        "    LOCAL.mkdir(exist_ok=True)\n",
        "    for gguf in FILES:\n",
        "        q = Queue()\n",
        "        p = Process(target=worker, args=(REPO, gguf, PROMPTS, q))\n",
        "        p.start()\n",
        "        p.join()         # attend la fin ➜ RAM rendue\n",
        "        if not q.empty():\n",
        "            model, layers, outs = q.get()\n",
        "            speed = sum(o[\"tok\"]/o[\"sec\"] for o in outs)/len(outs)\n",
        "            print(f\"✅ {model}  |  GPU layers={layers}  |  {speed:.2f} tok/s\")\n",
        "        else:\n",
        "            print(f\"❌ {gguf} a échoué\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # désactive le warning py\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    run_all()"
      ],
      "metadata": {
        "id": "KR8N395t2FZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Créer un pipeline de traduction pour mistral instruct :"
      ],
      "metadata": {
        "id": "9URvY5XJIYOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wqLYDNUdIc2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deepseek : devenir humble, un avenir pour l'IA ?"
      ],
      "metadata": {
        "id": "WhjBKraqH28I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installez d’abord les dépendances (dans une cellule Jupyter)\n",
        "!pip install -q llama-cpp-python huggingface-hub\n",
        "\n",
        "# Puis le script complet :\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# 1) Téléchargement du modèle GGUF (ex : quantification 3 bits)\n",
        "repo_id   = \"TheBloke/deepseek-coder-6.7B-instruct-GGUF\"\n",
        "file_name = \"deepseek-coder-6.7b-instruct.Q3_K_S.gguf\"\n",
        "local_dir = \"models\"\n",
        "\n",
        "import os\n",
        "os.makedirs(local_dir, exist_ok=True)\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=repo_id,\n",
        "    filename=file_name,\n",
        "    local_dir=local_dir\n",
        ")\n",
        "\n",
        "# 2) Chargement du modèle avec llama-cpp-python\n",
        "#    n_gpu_layers=0 → full CPU, ou mettez >0 si vous avez un GPU CUDA\n",
        "llm = Llama(\n",
        "    model_path = model_path,\n",
        "    n_ctx       = 2048,\n",
        "    n_gpu_layers= 0,\n",
        "    temperature = 0.7\n",
        ")\n",
        "\n",
        "# 3) Génération d’exemple\n",
        "prompt = (\n",
        "    \"### Instruction:\\n\"\n",
        "    \"Que proposes-tu en faveur de l'écologie ?  .\\n\"\n",
        "    \"### Response:\\n\"\n",
        ")\n",
        "resp = llm(prompt, max_tokens=500)\n",
        "\n",
        "# 4) Affichage de la réponse\n",
        "print(resp[\"choices\"][0][\"text\"].strip())\n"
      ],
      "metadata": {
        "id": "p7nTE_ctDDwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installez d’abord les dépendances (dans une cellule Jupyter)\n",
        "!pip install -q llama-cpp-python huggingface-hub pandas\n",
        "\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# ───── Config ─────\n",
        "REPO_ID   = \"TheBloke/deepseek-coder-6.7B-instruct-GGUF\"\n",
        "LOCAL_DIR = Path(\"models\")\n",
        "LOCAL_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Liste des fichiers GGUF à comparer\n",
        "QUANT_FILES = {\n",
        "    \"Q4 (4 bits)\": \"deepseek-coder-6.7b-instruct.Q4_K_M.gguf\",\n",
        "    \"Q5 (5 bits)\": \"deepseek-coder-6.7b-instruct.Q5_K_M.gguf\",\n",
        "    \"Q6 (6 bits)\": \"deepseek-coder-6.7b-instruct.Q6_K.gguf\",\n",
        "}\n",
        "\n",
        "# Prompt de test (modifiable)\n",
        "PROMPT = (\n",
        "    \"### Instruction:\\n\"\n",
        "    \"Que proposes-tu en faveur de l'écologie au quotidien ?\\n\"\n",
        "    \"### Response:\\n\"\n",
        ")\n",
        "\n",
        "# Nombre de tokens max à générer\n",
        "MAX_TOKENS = 200\n",
        "\n",
        "# GPU layers (0 = full CPU)\n",
        "# Changez ce paramètre si vous voulez déporter des couches sur GPU\n",
        "N_GPU_LAYERS = 0\n",
        "\n",
        "# ───── Boucle de comparaison ─────\n",
        "results = []\n",
        "\n",
        "for label, filename in QUANT_FILES.items():\n",
        "    # 1) Téléchargement si nécessaire\n",
        "    local_path = LOCAL_DIR / filename\n",
        "    if not local_path.exists():\n",
        "        print(f\"⬇️ Téléchargement de {filename} …\")\n",
        "        hf_hub_download(\n",
        "            repo_id=REPO_ID,\n",
        "            filename=filename,\n",
        "            local_dir=str(LOCAL_DIR)\n",
        "        )\n",
        "    else:\n",
        "        print(f\"✅ {filename} déjà présent\")\n",
        "\n",
        "    # 2) Chargement du modèle\n",
        "    print(f\"\\n🚀 Chargement du modèle {label} ({filename}) …\")\n",
        "    llm = Llama(\n",
        "        model_path   = str(local_path),\n",
        "        n_ctx        = 2048,\n",
        "        n_gpu_layers = N_GPU_LAYERS,\n",
        "        temperature  = 0.7\n",
        "    )\n",
        "\n",
        "    # 3) Génération & mesure du temps\n",
        "    t0 = time.perf_counter()\n",
        "    resp = llm(PROMPT, max_tokens=MAX_TOKENS)\n",
        "    dt = time.perf_counter() - t0\n",
        "\n",
        "    text = resp[\"choices\"][0][\"text\"].strip()\n",
        "    # estimation du nombre de tokens produits\n",
        "    n_tokens = len(text.split())\n",
        "\n",
        "    speed = n_tokens / dt if dt > 0 else None\n",
        "    print(f\"💬 Réponse ({label}) ({n_tokens} tokens en {dt:.2f}s) → {speed:.1f} tok/s\")\n",
        "    print(text, \"\\n\" + \"-\"*60 + \"\\n\")\n",
        "\n",
        "    # 4) Collecte des résultats\n",
        "    results.append({\n",
        "        \"quantification\": label,\n",
        "        \"tokens\": n_tokens,\n",
        "        \"time_s\": round(dt, 2),\n",
        "        \"tok_per_s\": round(speed, 1),\n",
        "        \"response\": text.replace(\"\\n\", \" \")\n",
        "    })\n",
        "\n",
        "# ───── Synthèse ─────\n",
        "df = pd.DataFrame(results)\n",
        "print(\"=== Comparatif des quantifications ===\")\n",
        "print(df[[\"quantification\",\"tokens\",\"time_s\",\"tok_per_s\"]])\n",
        "\n",
        "# Enregistrer dans un CSV si besoin\n",
        "df.to_csv(\"quant_compare_results.csv\", index=False)\n",
        "print(\"\\n✅ Exporté vers quant_compare_results.csv\")\n"
      ],
      "metadata": {
        "id": "dV3lZnZeL3Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut remarquer que **Deepseek** est particulièrement ** humble** et que plus le nombre de milliards de paramètres **augmente**, **plus **il devient plus **poli**.\n",
        "L'utilisation de **Deepseek**sur des ressources **entièrement CPU** , avec un temps de réponse de **60 seconde  à1 minutes 30**, comparable à l'utilisation GPU est **une remarquable avancée**."
      ],
      "metadata": {
        "id": "OSIXuEa0P2Jw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSGJvMXLO8NK"
      },
      "source": [
        "# Partie 4: fonctionnalités :après avoir choisi le modèle plus utile ?\n",
        "\n",
        "Zephyr va donc être **particulièrement utilisé** ici par la suite , il semble cohérent , utile et donné des réponses assez efficaces  !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "657o0CPbyXPz"
      },
      "source": [
        "# But 1: savoir rendre l'information plus accessible, en la résumant efficacement, avec 4 critères.\n",
        "\n",
        "En effet, nous avons utilisé en premier lieu un prompt qui résume efficacement les informations\n",
        "\n",
        "\n",
        "1.   le **contenu** du panier\n",
        "2.  connaître le **coût**  \n",
        "3. savoir où chercher le panier\n",
        "4. Savoir quand acheter le panier\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.OPT 125M :"
      ],
      "metadata": {
        "id": "4Ar4rPuZLmUM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DJrftwLBAtU"
      },
      "outputs": [],
      "source": [
        "index = 0\n",
        "\n",
        "article = dataset['test'][index]['texte']\n",
        "# On remplace les mots susrlignés.\n",
        "summary = dataset['test'][index]['texte']\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Résume en fonction de quoi,coût, où, quand\n",
        "\n",
        "### Entrée:\n",
        "{article}\n",
        "\n",
        "### Résumer :\n",
        "\"\"\"\n",
        "\n",
        "# oN va mettre les inputs\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOkgikw9dEHF"
      },
      "source": [
        "On peut observer que le résumé par **OPT125m** a l'avantage d'être assez **courts** ! Ainsi,on ne l'a que aidé en créant juste une \"tokenization\" en amont. Nous allons comparer avec le Mistral instruct 2bits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7Gigu1yBi6a"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Zephyr :"
      ],
      "metadata": {
        "id": "28STz5jDYX7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "\n",
        "# Chargement du tokenizer et modèle Zephyr quantifié (4 bits)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/zephyr-7B-beta-GPTQ\", use_fast=True)\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    \"TheBloke/zephyr-7B-beta-GPTQ\",\n",
        "    device_map=\"auto\",\n",
        "    use_safetensors=True,\n",
        "    trust_remote_code=True,\n",
        "    use_triton=True\n",
        ")\n",
        "\n",
        "# Chargement de l'exemple\n",
        "index = 0\n",
        "article = dataset['test'][index]['texte']\n",
        "summary = dataset['test'][index]['texte']  # ou utiliser un champ résumé de référence\n",
        "\n",
        "# Création du prompt au format instruct\n",
        "prompt = f\"[INST] Résume le texte suivant en précisant : quoi, coût, où, quand.\\n\\n{article} [/INST]\"\n",
        "\n",
        "# Préparation des inputs\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "\n",
        "# Génération\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "# Affichage formaté\n",
        "dash_line = '-' * 100\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n"
      ],
      "metadata": {
        "id": "r31846Q9YaPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9igw9u7gSkp"
      },
      "source": [
        "2.Low rank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoOtLJRQHl9m"
      },
      "source": [
        "  2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMWoPTiftOSE"
      },
      "source": [
        "2. Comparaison avec bert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gb3GBaqPtRO5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "\n",
        "# Pipeline de summarization avec un modèle pré-entraîné (ex : t5-small ou bart)\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\")\n",
        "\n",
        "# Fonction pour résumer chaque texte\n",
        "def resume_texte(texte):\n",
        "    # Certains modèles ont une limite de tokens, on coupe si nécessaire\n",
        "    if len(texte.split()) > 512:\n",
        "        texte = \" \".join(texte.split()[:512])\n",
        "    summary = summarizer(texte, max_length=60, min_length=20, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# Création d'une nouvelle colonne avec les résumés\n",
        "df[\"résumé\"] = df[\"texte\"].apply(resume_texte)\n",
        "\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RttcfjRXuDH5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCzvrF2luEMc"
      },
      "source": [
        "3. Entraîner le lama à la conversation sur le contenu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWjqeMQ_vRDs"
      },
      "source": [
        "1.Conversation du coup pour un mot-clé.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpvTKRLKzh5i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd  # Importe la bibliothèque Pandas\n",
        "\n",
        "def chercher_par_sujet(df, sujet):\n",
        "    \"\"\"\n",
        "    Cherche les sources d'information qui traitent d'un sujet donné.\n",
        "    \"\"\"\n",
        "    resultats = []\n",
        "    for index, row in df.iterrows():\n",
        "        if sujet in row[\"sujets\"]:\n",
        "            resultats.append(row.to_dict())  # Convertit la ligne en dictionnaire\n",
        "    return resultats\n",
        "\n",
        "def afficher_resume(source):\n",
        "    \"\"\"\n",
        "    Affiche un résumé de la source d'information.\n",
        "    \"\"\"\n",
        "    print(f\"Source : {source['url']}\")\n",
        "    # Ici, tu peux ajouter une logique pour raccourcir le texte si nécessaire\n",
        "    print(f\"Résumé : {source['texte'][:200]}...\")  # Affiche les 200 premiers caractères\n",
        "\n",
        "def poser_question(df):\n",
        "    \"\"\"\n",
        "    Permet à l'utilisateur de poser une question et tente d'y répondre.\n",
        "    \"\"\"\n",
        "    question = input(\"Que souhaitez-vous savoir ? \")\n",
        "    mots_cles = question.lower().split()  # Convertit en minuscules et divise en mots\n",
        "\n",
        "    sources_pertinentes = []\n",
        "    for mot in mots_cles:\n",
        "        sources_pertinentes.extend(chercher_par_sujet(df, mot))\n",
        "\n",
        "    if sources_pertinentes:\n",
        "        print(\"\\nVoici les informations que j'ai trouvées :\")\n",
        "        for source in set(sources_pertinentes):  # Pour éviter les doublons\n",
        "            afficher_resume(source)\n",
        "    else:\n",
        "        print(\"\\nDésolé, je n'ai pas trouvé d'informations pertinentes.\")\n",
        "\n",
        "# 1. Création du DataFrame\n",
        "data = [\n",
        "    {\n",
        "        \"url\": \"https://www.reussir.fr/...\",\n",
        "        \"texte\": \"« Un concept innovant... » Le producteur alsacien Jean-Michel Obrecht...\",\n",
        "        \"type\": \"article\",\n",
        "        \"sujets\": [\"agriculture\", \"commerce local\", \"Strasbourg\"]\n",
        "    },\n",
        "    {\n",
        "        \"url\": \"https://www.dna.fr/politique/...\",\n",
        "        \"texte\": \"Gundershoffen Un vrai élan de solidarité pour l’épicerie sociale... \",\n",
        "        \"type\": \"article\",\n",
        "        \"sujets\": [\"solidarité\", \"épicerie sociale\", \"Gundershoffen\"]\n",
        "    },\n",
        "    {\n",
        "        \"url\": \"https://www.francebleu.fr/...\",\n",
        "        \"texte\": \"Solibio, la coopérative grossiste alsacienne bio, a le vent en poupe... \",\n",
        "        \"type\": \"article\",\n",
        "        \"sujets\": [\"bio\", \"coopérative\", \"Strasbourg\"]\n",
        "    },\n",
        "    {\n",
        "        \"url\": \"https://www.lespaniersdupetitlucien.fr/...\",\n",
        "        \"texte\": \"Vos questions, nos réponses. ------ Panier Classique ou Plus ? ------ Notre panier Classique est idéal...\",\n",
        "        \"type\": \"FAQ\",\n",
        "        \"sujets\": [\"paniers bio\", \"livraison\", \"abonnement\"]\n",
        "    },\n",
        "    # ... et ainsi de suite pour tes autres sources\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2. Exemple d'utilisation\n",
        "poser_question(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNjeYM_Z0nuI"
      },
      "source": [
        "# BUT 2: plus instruire sur la cause \"écologique \" en elle-même, avec des actions concrètes.\n",
        "\n",
        "Quel est mon impact carbone si je prends l’avion de Paris à Montréal ?\n",
        "\n",
        "Est-ce plus écologique de manger du poulet local ou du tofu importé ?\n",
        "\n",
        "Est-ce que l’usage d’un vélo électrique est réellement plus vert que le métro ?\n",
        "\n",
        "Quelle est la meilleure option pour chauffer mon appartement de façon écolo ?\n",
        "Qu’est-ce que l’effet de serre ?\n",
        "\n",
        "Quelle est la différence entre changement climatique et réchauffement climatique ?\n",
        "\n",
        "Donne-moi 5 actions concrètes pour protéger la biodiversité.\n",
        "\n",
        "Explique-moi simplement ce qu’est une zone morte dans l’océan.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "czbkjCAfmz9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPKdmYtb1D_8"
      },
      "source": [
        "## But 2: savoir répondre à une dizaine de questions pour les personnes précaires afin de les aider à choisir un lieu à budget serré.\n",
        "\n",
        "Le but est d'inciter ici les personnes à acheter plus \"local\" notamment en leur proposant différentes questions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Installer les dépendances\n",
        "!pip install -q transformers auto-gptq accelerate\n",
        "\n",
        "# 1. Installer les dépendances\n",
        "!pip install -q transformers auto-gptq accelerate\n",
        "\n",
        "# 2. Imports\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, TextStreamer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "import torch\n",
        "\n",
        "# Check CUDA availability and set device accordingly\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"CUDA not available, using CPU instead.\")\n",
        "\n",
        "\n",
        "# 3. Tes données originales\n",
        "data = [\n",
        "    {\"url\":\"https://www.reussir.fr/...\",\"texte\":\"Jean-Michel Obrecht a créé une boutique qui mélange produits locaux et objets de brocante...\",\"type\":\"article\",\"sujets\":[\"agriculture\",\"commerce local\",\"Strasbourg\"],\"reponses\":{1:\"Jean-Michel Obrecht a créé une boutique qui mélange produits locaux et objets de brocante pour diversifier son offre et attirer une clientèle variée.\"}},\n",
        "    {\"url\":\"https://www.dna.fr/politique/...\",\"texte\":\"L'épicerie sociale de Gundershoffen est une fierté pour l'intercommunalité...\",\"type\":\"article\",\"sujets\":[\"solidarité\",\"épicerie sociale\",\"Gundershoffen\"],\"reponses\":{2:\"L'épicerie sociale de Gundershoffen permet aux personnes précaires de faire leurs courses à un tarif très réduit et apporte une dimension humaine aux politiques économiques locales.\"}},\n",
        "    {\"url\":\"https://www.francebleu.fr/...\",\"texte\":\"Solibio, la coopérative grossiste alsacienne bio, prospère en fournissant les magasins et cantines de la région...\",\"type\":\"article\",\"sujets\":[\"bio\",\"coopérative\",\"Strasbourg\"],\"reponses\":{3:\"Solibio connaît le succès grâce à son modèle économique axé sur la restauration collective et les magasins bio, et à la demande soutenue pour les produits bio locaux.\"}},\n",
        "    {\"url\":\"https://www.lespaniersdupetitlucien.fr/...\",\"texte\":\"Les Paniers du Petit Lucien proposent un panier Classique pour 2 à 3 personnes et un panier Plus avec des articles plus rares...\",\"type\":\"FAQ\",\"sujets\":[\"paniers bio\",\"livraison\",\"abonnement\"],\"reponses\":{4:\"Les Paniers du Petit Lucien offrent un panier Classique, idéal pour 2 à 3 personnes, et un panier Plus, qui inclut des produits plus rares et convient à 3 ou 4 personnes.\"}},\n",
        "    {\"url\":\"https://cop1.fr/a-propos/\",\"texte\":\"Cop1 a été créée en réaction à la précarité étudiante et propose des paniers alimentaires, un accompagnement et des activités...\",\"type\":\"association\",\"sujets\":[\"aide alimentaire\",\"étudiants\",\"précarité\"],\"reponses\":{5:\"Cop1 apporte une aide alimentaire aux étudiants, un soutien pour leur projet professionnel et un accès à des activités culturelles et sportives.\"}},\n",
        "    {\"url\":\"https://www.neozone.org/...\",\"texte\":\"L'ordonnance verte de Strasbourg offre aux femmes enceintes un accès gratuit à des légumes bio pour les protéger des perturbateurs endocriniens...\",\"type\":\"article\",\"sujets\":[\"écologie\",\"santé\",\"Strasbourg\"],\"reponses\":{6:\"L'ordonnance verte de Strasbourg vise à protéger les femmes enceintes et leurs enfants de l'exposition aux perturbateurs endocriniens en leur fournissant des paniers de légumes bio et des ateliers d'information.\"}},\n",
        "    {\"url\":\"https://strasinfo.fr/...\",\"texte\":\"Le Petit marché de Cronenbourg est un marché hybride qui mêle vente de produits alimentaires locaux et animations conviviales...\",\"type\":\"article\",\"sujets\":[\"marché local\",\"Cronenbourg\",\"alimentation\"],\"reponses\":{7:\"Le Petit marché de Cronenbourg est un marché hebdomadaire qui combine la vente de produits alimentaires de circuits courts avec des animations pour les habitants du quartier.\"}},\n",
        "    {\"url\":\"https://www.vertici.fr/\",\"texte\":\"Vert Ici est un concept de restauration rapide qui propose des salades, sandwichs, jus et smoothies sur mesure, avec des produits frais et de qualité...\",\"type\":\"site web\",\"sujets\":[\"restauration\",\"alimentation saine\",\"Strasbourg\"],\"reponses\":{8:\"Vert Ici est un restaurant situé dans le centre commercial Aubette à Strasbourg, qui offre à ses clients la possibilité de composer leurs propres salades et sandwichs avec des ingrédients frais.\"}},\n",
        "    {\"url\":\"https://www.strasbourg.eu/...\",\"texte\":\"L'Atelier de la Réserve naturelle de Neuhof/Illkirch-Graffenstaden est une démarche participative pour concilier les activités humaines et la préservation de la biodiversité...\",\"type\":\"article\",\"sujets\":[\"environnement\",\"réserve naturelle\",\"participation citoyenne\"],\"reponses\":{9:\"L'Atelier de la Réserve naturelle de Neuhof/Illkirch-Graffenstaden a pour objectif de réunir citoyens, experts et élus pour réfléchir à la manière de concilier les activités humaines avec la protection de la réserve naturelle.\"}},\n",
        "    {\"url\":\"https://zds.fr/presentation/\",\"texte\":\"Zéro Déchet Strasbourg est une association qui sensibilise à la réduction et à la gestion durable des déchets et mène des actions pour faciliter les modes de vie zéro déchet...\",\"type\":\"association\",\"sujets\":[\"zéro déchet\",\"environnement\",\"Strasbourg\"],\"reponses\":{10:\"Zéro Déchet Strasbourg a pour mission de sensibiliser à la problématique des déchets, de promouvoir des modes de vie zéro déchet et d'encourager les actions des citoyens et des décideurs.\"}}\n",
        "]\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 4. Les 10 questions\n",
        "questions = {\n",
        "    1: \"Quel est le concept innovant mis en place par Jean-Michel Obrecht pour vendre ses produits ?\",\n",
        "    2: \"Comment l'épicerie sociale de Gundershoffen aide-t-elle les personnes en situation de précarité ?\",\n",
        "    3: \"Pourquoi la coopérative Solibio connaît-elle une croissance constante malgré les difficultés rencontrées par le secteur du bio ?\",\n",
        "    4: \"Quelles sont les deux formules de paniers proposées par Les Paniers du Petit Lucien et quelles différences y a-t-il entre elles ?\",\n",
        "    5: \"De quelle manière l'association Cop1 apporte-t-elle son soutien aux étudiants confrontés à la précarité ?\",\n",
        "    6: \"Quel est l'objectif principal de l'ordonnance verte mise en place par la ville de Strasbourg ?\",\n",
        "    7: \"Qu'est-ce que le Petit marché de Cronenbourg et quels types de produits et d'animations peut-on y trouver ?\",\n",
        "    8: \"Où se trouve le restaurant Vert Ici à Strasbourg et quel est le concept de son offre culinaire ?\",\n",
        "    9: \"Quel est le but de la création de l'Atelier de la Réserve naturelle de Neuhof/Illkirch-Graffenstaden ?\",\n",
        "   10: \"Quelles sont les principales actions menées par l'association Zéro Déchet Strasbourg ?\"\n",
        "}\n",
        "\n",
        "# 5. Charger le modèle GPTQ\n",
        "model_id = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    model_id,\n",
        "    device_map=\"auto\",        # répartit sur tous les dispositifs disponibles\n",
        "    use_safetensors=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "\n",
        "# 6. Boucle : pour chaque question, on génère puis on affiche la réponse stockée\n",
        "for q_id, q_text in questions.items():\n",
        "    print(f\"\\n🧠 Question {q_id}: {q_text}\")\n",
        "    # on peut ici demander au modèle de paraphraser ou valider, mais on connaît déjà la réponse :\n",
        "    answer = df.loc[df[\"reponses\"].apply(lambda d: q_id in d), \"reponses\"].item()[q_id]\n",
        "    print(f\"✅ Réponse : {answer}\")\n"
      ],
      "metadata": {
        "id": "BiF5nCJkkrri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GL1i3f41IQu"
      },
      "outputs": [],
      "source": [
        "# ─── 1. Installer les dépendances ───────────────────────────────────────────────\n",
        "!pip install -q transformers auto-gptq accelerate bitsandbytes pandas\n",
        "\n",
        "# ─── 2. Imports ────────────────────────────────────────────────────────────────\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM, GPTQQuantizationConfig\n",
        "\n",
        "# ─── 3. Vos données originales et réponses ────────────────────────────────────\n",
        "data = [\n",
        "    {\"url\":\"https://www.reussir.fr/...\",\"texte\":\"Jean-Michel Obrecht a créé une boutique qui mélange produits locaux et objets de brocante...\",\"type\":\"article\",\"sujets\":[\"agriculture\",\"commerce local\",\"Strasbourg\"],\"reponses\":{1:\"Jean-Michel Obrecht a créé une boutique qui mélange produits locaux et objets de brocante pour diversifier son offre et attirer une clientèle variée.\"}},\n",
        "    {\"url\":\"https://www.dna.fr/politique/...\",\"texte\":\"L'épicerie sociale de Gundershoffen est une fierté pour l'intercommunalité...\",\"type\":\"article\",\"sujets\":[\"solidarité\",\"épicerie sociale\",\"Gundershoffen\"],\"reponses\":{2:\"Par exemple, dans cette épicerie sociale : un panier type dont la valeur en magasin (Leclerc) serait d’environ 30 € ne coûte que 6–8 € à l’épicerie sociale de Gundershoffen (soit une remise de l’ordre de 70–80 %).\"}},\n",
        "    {\"url\":\"https://www.francebleu.fr/...\",\"texte\":\"Solibio, la coopérative grossiste alsacienne bio, prospère en fournissant les magasins et cantines de la région...\",\"type\":\"article\",\"sujets\":[\"bio\",\"coopérative\",\"Strasbourg\"],\"reponses\":{3:\"Solibio connaît le succès grâce à son modèle économique axé sur la restauration collective et les magasins bio, et à la demande soutenue pour les produits bio locaux.\"}},\n",
        "    {\"url\":\"https://www.lespaniersdupetitlucien.fr/...\",\"texte\":\"Les Paniers du Petit Lucien proposent un panier Classique pour 2 à 3 personnes et un panier Plus avec des articles plus rares...\",\"type\":\"FAQ\",\"sujets\":[\"paniers bio\",\"livraison\",\"abonnement\"],\"reponses\":{4:\"Les Paniers du Petit Lucien offrent un panier Classique, idéal pour 2 à 3 personnes, et un panier Plus, qui inclut des produits plus rares et convient à 3 ou 4 personnes.\"}},\n",
        "    {\"url\":\"https://cop1.fr/a-propos/\",\"texte\":\"Cop1 a été créée en réaction à la précarité étudiante et propose des paniers alimentaires, un accompagnement et des activités...\",\"type\":\"association\",\"sujets\":[\"aide alimentaire\",\"étudiants\",\"précarité\"],\"reponses\":{5:\"Cop1 apporte une aide alimentaire aux étudiants, un soutien pour leur projet professionnel et un accès à des activités culturelles et sportives.\"}},\n",
        "    {\"url\":\"https://www.neozone.org/...\",\"texte\":\"L'ordonnance verte de Strasbourg offre aux femmes enceintes un accès gratuit à des légumes bio pour les protéger des perturbateurs endocriniens...\",\"type\":\"article\",\"sujets\":[\"écologie\",\"santé\",\"Strasbourg\"],\"reponses\":{6:\"L'ordonnance verte de Strasbourg vise à protéger les femmes enceintes et leurs enfants de l'exposition aux perturbateurs endocriniens en leur fournissant des paniers de légumes bio et des ateliers d'information.\"}},\n",
        "    {\"url\":\"https://strasinfo.fr/...\",\"texte\":\"Le Petit marché de Cronenbourg est un marché hybride qui mêle vente de produits alimentaires locaux et animations conviviales...\",\"type\":\"article\",\"sujets\":[\"marché local\",\"Cronenbourg\",\"alimentation\"],\"reponses\":{7:\"Le Petit marché de Cronenbourg est un marché hebdomadaire qui combine la vente de produits alimentaires de circuits courts avec des animations pour les habitants du quartier.\"}},\n",
        "    {\"url\":\"https://www.vertici.fr/\",\"texte\":\"Vert Ici est un concept de restauration rapide qui propose des salades, sandwichs, jus et smoothies sur mesure, avec des produits frais et de qualité...\",\"type\":\"site web\",\"sujets\":[\"restauration\",\"alimentation saine\",\"Strasbourg\"],\"reponses\":{8:\"En moyenne, comptez 10–12 € pour une salade/sandwich chez Vert Ici, contre 3–5 € pour les mêmes ingrédients achetés chez Leclerc.\"}},\n",
        "    {\"url\":\"https://www.strasbourg.eu/...\",\"texte\":\"L'Atelier de la Réserve naturelle de Neuhof/Illkirch-Graffenstaden est une démarche participative pour concilier les activités humaines et la préservation de la biodiversité...\",\"type\":\"article\",\"sujets\":[\"environnement\",\"réserve naturelle\",\"participation citoyenne\"],\"reponses\":{9:\"L'Atelier de la Réserve naturelle de Neuhof/Illkirch-Graffenstaden a pour objectif de réunir citoyens, experts et élus pour réfléchir à la manière de concilier les activités humaines avec la protection de la réserve naturelle.\"}},\n",
        "    {\"url\":\"https://zds.fr/presentation/\",\"texte\":\"Zéro Déchet Strasbourg est une association qui sensibilise à la réduction et à la gestion durable des déchets et mène des actions pour faciliter les modes de vie zéro déchet...\",\"type\":\"association\",\"sujets\":[\"zéro déchet\",\"environnement\",\"Strasbourg\"],\"reponses\":{10:\"Zéro Déchet Strasbourg a pour mission de sensibiliser à la problématique des déchets, de promouvoir des modes de vie zéro déchet et d'encourager les actions des citoyens et des décideurs.\"}}\n",
        "]\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# ─── 4. Questions préconfigurées ─────────────────────────────────────────────\n",
        "questions = {\n",
        "    1: \"Quel est le concept innovant mis en place par Jean-Michel Obrecht pour vendre ses produits ?\",\n",
        "    2: \"Comment l'épicerie sociale de Gundershoffen aide-t-elle les personnes en situation de précarité ?\",\n",
        "    3: \"Pourquoi la coopérative Solibio connaît-elle une croissance constante malgré les difficultés rencontrées par le secteur du bio ?\",\n",
        "    4: \"Quelles sont les deux formules de paniers proposées par Les Paniers du Petit Lucien et quelles différences y a-t-il entre elles ?\",\n",
        "    5: \"De quelle manière l'association Cop1 apporte-t-elle son soutien aux étudiants confrontés à la précarité ?\",\n",
        "    6: \"Quel est l'objectif principal de l'ordonnance verte mise en place par la ville de Strasbourg ?\",\n",
        "    7: \"Qu'est-ce que le Petit marché de Cronenbourg et quels types de produits et d'animations peut-on y trouver ?\",\n",
        "    8: \"Où se trouve le restaurant Vert Ici à Strasbourg et quel est le concept de son offre culinaire ?\",\n",
        "    9: \"Quel est le but de la création de l'Atelier de la Réserve naturelle de Neuhof/Illkirch-Graffenstaden ?\",\n",
        "   10: \"Quelles sont les principales actions menées par l'association Zéro Déchet Strasbourg ?\"\n",
        "}\n",
        "\n",
        "# ─── 5. Charger et quantifier en 2-bit si nécessaire ──────────────────────────\n",
        "source_model = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(source_model, use_fast=True)\n",
        "\n",
        "quant_config = GPTQQuantizationConfig(\n",
        "    bits=2,\n",
        "    use_safetensors=True,\n",
        "    warmup_steps=0\n",
        ")\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_pretrained(\n",
        "    source_model,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# ─── 6. Fonction IA pour déterminer quelle question l'utilisateur pose ─────\n",
        "def select_question_id(user_text: str) -> int:\n",
        "    q_list = \"\\n\".join(f\"{i}. {q}\" for i, q in questions.items())\n",
        "    prompt = (\n",
        "        f\"Questions :\\n{q_list}\\n\\n\"\n",
        "        f\"Utilisateur : « {user_text} »\\n\"\n",
        "        \"Retourne uniquement le numéro de la question correspondante.\"\n",
        "    )\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(**inputs, max_new_tokens=2)\n",
        "    pred = tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
        "    return int(pred) if pred.isdigit() else None\n",
        "\n",
        "# ─── 7. Boucle interactive ───────────────────────────────────────────────────\n",
        "print(\"Entrez 'q' pour quitter.\")\n",
        "while True:\n",
        "    user = input(\"Pose ta question : \")\n",
        "    if user.lower() == \"q\":\n",
        "        print(\"Au revoir !\")\n",
        "        break\n",
        "    qid = select_question_id(user)\n",
        "    if qid in questions:\n",
        "        # récupérer la réponse originale\n",
        "        answer = df.loc[df[\"reponses\"].apply(lambda d: qid in d), \"reponses\"].item()[qid]\n",
        "        print(f\"\\n❓ {questions[qid]}\\n💬 {answer}\\n\")\n",
        "    else:\n",
        "        print(\"❌ Je n'ai pas trouvé de correspondance, réessaie.\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LM_xZhksaIFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtuVf4eE9ypp"
      },
      "source": [
        "# **PARTIE Essi**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCgeISLXzyvM"
      },
      "outputs": [],
      "source": [
        "# Les bibliothèques nécessaires\n",
        "import json\n",
        "from collections import defaultdict # Permet d'initialiser des valeurs par défaut pour les nouvelles clés\n",
        "\n",
        "!pip install gradio # Important à installer car ce n'est pas déjà prédéfinie\n",
        "import gradio as gr # Pour l'interface graphique Web"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASXNvvjA0Qz4"
      },
      "source": [
        "# **Première partie** : on importe le JSON et on fait un bref appercue\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0iae9nb0Ytz"
      },
      "outputs": [],
      "source": [
        "#On importe le fichier\n",
        "with open('/content/CleanAndStrutured.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# On affiche les clés et leurs valeurs\n",
        "if isinstance(data, dict):\n",
        "    print(\"Aperçu des clés et valeurs :\")\n",
        "    for key, value in data.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "elif isinstance(data, list):\n",
        "    print(\"Aperçu des objets dans la liste :\")\n",
        "    for idx, item in enumerate(data[:5]):  # Montre les 5 premiers objets\n",
        "        print(f\"Objet {idx + 1}:\")\n",
        "        for key, value in item.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "else:\n",
        "    print(\"Type de données non supporté.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIJXThc_0hWs"
      },
      "source": [
        "# **Deuxième partie**: conversation par mot-clé (le back-end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRgv6_Lf0rog"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/CleanAndStrutured.json'\n",
        "keywords_path = '/content/keywords_config.json'\n",
        "\n",
        "class SimpleQASystem: # On definit une class nommé \"SimpleQASystem\" le nom fait enfait réference à 'Simple Question Answer Stystèm', conçue pour être un système simple de questions-réponses basé sur des mots-clés.\n",
        "    def __init__(self, data_path, keywords_path):\n",
        "        # Chargement des données\n",
        "        with open(data_path, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        # Chargement de la configuration des mots-clés\n",
        "        with open(keywords_path, 'r', encoding='utf-8') as f:\n",
        "            self.keywords = json.load(f)\n",
        "\n",
        "        # Construction de l'index pour une recherche rapide\n",
        "        self.index = self._build_index()\n",
        "\n",
        "    def _build_index(self):\n",
        "        \"\"\"\n",
        "        Construit un index inverse pour accélérer les recherches.\n",
        "        \"\"\"\n",
        "        index = defaultdict(list)\n",
        "\n",
        "        for idx, entry in enumerate(self.data):\n",
        "            # Combine plusieurs champs textuels pour l'indexation\n",
        "            text_fields = [\n",
        "                entry.get('description', ''),\n",
        "                entry.get('type_de_produits', ''),\n",
        "                entry.get('categorie', '')\n",
        "            ]\n",
        "            search_text = ' '.join(text_fields).lower()\n",
        "\n",
        "            # Indexation par localisation\n",
        "            loc = entry.get('localisation', '').lower()\n",
        "            if loc:\n",
        "                index[loc].append(idx)\n",
        "\n",
        "            # Indexation par catégories et sous-catégories\n",
        "            for category in self.keywords['categories']:\n",
        "                # Mots-clés principaux\n",
        "                for kw in self.keywords['categories'][category]['keywords']:\n",
        "                    if kw in search_text:\n",
        "                        index[kw].append(idx)\n",
        "\n",
        "                # Sous-catégories\n",
        "                for subcat in self.keywords['categories'][category]['subcategories']:\n",
        "                    for sub_kw in self.keywords['categories'][category]['subcategories'][subcat]:\n",
        "                        if sub_kw in search_text:\n",
        "                            index[sub_kw].append(idx)\n",
        "\n",
        "        return index\n",
        "\n",
        "    def _extract_address(self, entry):\n",
        "        \"\"\"\n",
        "        Formate une adresse complète à partir des champs disponibles.\n",
        "        \"\"\"\n",
        "        address_components = [\n",
        "            entry.get('adresse', '').strip(),\n",
        "            entry.get('code_postal', '').strip(),\n",
        "            entry.get('ville', '').strip()\n",
        "        ]\n",
        "\n",
        "        # Filtre les composants vides et les combine\n",
        "        full_address = ', '.join(filter(None, address_components))\n",
        "\n",
        "        return full_address if full_address else \"Adresse non renseignée\"\n",
        "\n",
        "    def _calculate_relevance(self, entry, question_keywords):\n",
        "        \"\"\"\n",
        "        Calcule un score de pertinence pour une entrée.\n",
        "\n",
        "        Args:\n",
        "            entry (dict): Entrée de données\n",
        "            question_keywords (set): Mots-clés de la question\n",
        "\n",
        "        Returns:\n",
        "            tuple: Score de pertinence (pour le tri)\n",
        "        \"\"\"\n",
        "        entry_text = f\"{entry['description']} {entry['type_de_produits']}\".lower()\n",
        "\n",
        "        # Nombre de mots-clés correspondants\n",
        "        keyword_matches = sum(\n",
        "            1 for kw in question_keywords\n",
        "            if kw in entry_text\n",
        "        )\n",
        "\n",
        "        # Longueur du texte (priorité aux descriptions plus complètes)\n",
        "        text_length = len(entry['description'])\n",
        "\n",
        "        return (-keyword_matches, -text_length)  # Tri décroissant\n",
        "\n",
        "    def find_best_match(self, question):\n",
        "        \"\"\"\n",
        "        Trouve les meilleures réponses pour une question.\n",
        "\n",
        "        Args:\n",
        "            question (str): Question posée par l'utilisateur\n",
        "\n",
        "        Returns:\n",
        "            list: Liste des 3 meilleures réponses formatées\n",
        "        \"\"\"\n",
        "        question_lower = question.lower()\n",
        "        matched_indices = set()\n",
        "        question_keywords = set()\n",
        "\n",
        "        # 1. Détection des mots-clés de localisation\n",
        "        loc_matches = []\n",
        "        for loc_kw in self.keywords['localisation']['strasbourg']:\n",
        "            if loc_kw in question_lower:\n",
        "                loc_matches.extend(self.index.get(loc_kw, []))\n",
        "                question_keywords.add(loc_kw)\n",
        "\n",
        "        # 2. Détection des mots-clés thématiques\n",
        "        for category in self.keywords['categories']:\n",
        "            # Mots-clés principaux\n",
        "            for kw in self.keywords['categories'][category]['keywords']:\n",
        "                if kw in question_lower:\n",
        "                    matched_indices.update(self.index.get(kw, []))\n",
        "                    question_keywords.add(kw)\n",
        "\n",
        "            # Sous-catégories\n",
        "            for subcat in self.keywords['categories'][category]['subcategories']:\n",
        "                for sub_kw in self.keywords['categories'][category]['subcategories'][subcat]:\n",
        "                    if sub_kw in question_lower:\n",
        "                        matched_indices.update(self.index.get(sub_kw, []))\n",
        "                        question_keywords.add(sub_kw)\n",
        "\n",
        "        # 3. Filtrage par localisation si spécifiée\n",
        "        if loc_matches:\n",
        "            matched_indices.intersection_update(loc_matches)\n",
        "\n",
        "        # 4. Récupération et tri des résultats\n",
        "        raw_results = [self.data[idx] for idx in matched_indices]\n",
        "\n",
        "        # Tri par pertinence\n",
        "        raw_results.sort(key=lambda x: self._calculate_relevance(x, question_keywords))\n",
        "\n",
        "        # Formatage des résultats finaux\n",
        "        formatted_results = []\n",
        "        for entry in raw_results[:3]:  # On garde seulement les 3 meilleurs\n",
        "            formatted_results.append({\n",
        "                'titre': entry.get('description', '').split('.')[0][:50] + '...',  # Première phrase tronquée\n",
        "                'description': entry.get('description', ''),\n",
        "                'adresse_complete': self._extract_address(entry),\n",
        "                'type': entry.get('type_de_produits', ''),\n",
        "                'categorie': entry.get('categorie', ''),\n",
        "                'url': entry.get('url', '')\n",
        "            })\n",
        "\n",
        "        return formatted_results\n",
        "\n",
        "# Instanciation du système Qestion Answer\n",
        "qa_system = SimpleQASystem(data_path, keywords_path)\n",
        "\n",
        "def predict(question):\n",
        "    results = qa_system.find_best_match(question)\n",
        "    if results:\n",
        "        output = \"\"\n",
        "        for i, result in enumerate(results):\n",
        "            output += f\"**Réponse {i+1}:**\\n\"\n",
        "            output += f\"- **Titre:** {result['titre']}\\n\"\n",
        "            output += f\"- **Description:** {result['description']}\\n\"\n",
        "            output += f\"- **Adresse:** {result['adresse_complete']}\\n\"\n",
        "            output += f\"- **Type:** {result['type']}\\n\"\n",
        "            output += f\"- **Catégorie:** {result['categorie']}\\n\"\n",
        "            if result['url']:\n",
        "                output += f\"- **URL:** {result['url']}\\n\"\n",
        "            output += \"\\n\"\n",
        "        return output\n",
        "    else:\n",
        "        return \"Aucun résultat pertinent trouvé.\"\n",
        "\n",
        "frequent_questions = [\n",
        "    \"Où-est ce que je peux des paniers légumes à tarif étudiant à Strasbourg ?\",\n",
        "    \"Où-est ce que je peux trouver des restaurants bio à Strasbourg ?\",\n",
        "    \"Peux-tu me conseiller des épiceries solidaires à Strasbourg ?\",\n",
        "    \"Où trouver des paniers bio à Strasbourg ?\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-_mwIycGWso"
      },
      "source": [
        "# **Troisième partie:**  Jeu du Budget Vert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px_eIf-UTOqw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scxMjIS408LC"
      },
      "outputs": [],
      "source": [
        "ACTIVITES = {\n",
        "    \"Cinéma\": 10,\n",
        "    \"Musique\": 5,\n",
        "    \"Concerts\": 20,\n",
        "    \"Festivals\": 30,\n",
        "    \"Musées\": 5,\n",
        "    \"Expositions\": 5,\n",
        "    \"Manga\": 5,\n",
        "    \"Animé\": 5,\n",
        "    \"Jeux de société\": 10,\n",
        "    \"Dessin\": 5,\n",
        "    \"Jeux vidéo\": 25,\n",
        "    \"Réseaux sociaux\": 10,\n",
        "    \"YouTube\": 10,\n",
        "    \"Sport\": 15,\n",
        "    \"Cuisine\": 10,\n",
        "    \"Vacances\": 45,\n",
        "    \"École\": 15,\n",
        "    \"Travail\": 10,\n",
        "}\n",
        "\n",
        "BUDGET_INITIAL = 50  # Budget carbone initial pour le week-end\n",
        "\n",
        "def ajouter_ou_retirer_activite(activite_nom, budget_restant, activites_choisies_liste, activites_choisies_texte):\n",
        "    \"\"\"Fonction pour ajouter ou retirer une activité et mettre à jour le budget.\"\"\"\n",
        "    cout = ACTIVITES.get(activite_nom, 0)\n",
        "    if activite_nom in activites_choisies_liste:\n",
        "        nouvelles_activites = [a for a in activites_choisies_liste if a != activite_nom]\n",
        "        nouveau_budget = budget_restant + cout\n",
        "    elif budget_restant >= cout:\n",
        "        nouvelles_activites = activites_choisies_liste + [activite_nom]\n",
        "        nouveau_budget = budget_restant - cout\n",
        "    else:\n",
        "        return budget_restant, activites_choisies_liste, activites_choisies_texte + f\"\\nBudget insuffisant pour {activite_nom} !\"\n",
        "\n",
        "    nouveau_texte = \"\\n- \".join([f\"{a} ({ACTIVITES[a]})\" for a in nouvelles_activites])\n",
        "    return nouveau_budget, nouvelles_activites, gr.Markdown(f\"**Activités choisies :**\\n- {nouveau_texte}\")\n",
        "\n",
        "def terminer_jeu(budget_restant, activites_choisies_liste):\n",
        "    \"\"\"Fonction pour afficher les résultats du jeu.\"\"\"\n",
        "    cout_total = BUDGET_INITIAL - budget_restant\n",
        "    message = f\"## Bilan de votre week-end éco-responsable :\\n\\n\"\n",
        "    message += f\"Votre budget initial était de **{BUDGET_INITIAL}** points carbone.\\n\\n\"\n",
        "    if activites_choisies_liste:\n",
        "        message += \"Vous avez choisi les activités suivantes :\\n- \"\n",
        "        message += \"\\n- \".join([f\"{a} ({ACTIVITES[a]})\" for a in activites_choisies_liste]) + \"\\n\\n\"\n",
        "        message += f\"Le coût total de vos activités est de **{cout_total}** points carbone.\\n\\n\"\n",
        "    else:\n",
        "        message += \"Vous n'avez choisi aucune activité.\\n\\n\"\n",
        "\n",
        "    if budget_restant >= 0:\n",
        "        message += f\"Il vous reste **{budget_restant}** points carbone. Bravo pour votre week-end potentiellement sobre en carbone !\"\n",
        "    else:\n",
        "        message += f\"Vous avez dépassé votre budget de **{-budget_restant}** points carbone. Essayez de faire des choix plus légers la prochaine fois !\"\n",
        "    return gr.Markdown(message, visible=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03qPg-D2eW8x"
      },
      "source": [
        "# **Sous partie 1** : Data pour la locatisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiC96182emt0"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    {\n",
        "        \"localisation\": {\n",
        "            \"contacts\": \"non renseigné\",\n",
        "            \"type_de_produits\": \"Produits locaux, fruits, légumes, objets vintage\",\n",
        "            \"categorie\": \"Commerce local\",\n",
        "            \"update\": \"2023-01-15\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"url\": \"https://www.dna.fr/politique/2025/01/15/un-vrai-elan-de-solidarite-pour-l-epicerie-sociale\",\n",
        "        \"description\": \"Un élan de solidarité pour l'épicerie sociale intercommunale de Gundershoffen.\",\n",
        "        \"localisation\": {\n",
        "            \"adresse\": \"Rue Principale, Gundershoffen, France\",\n",
        "            \"latitude\": 48.8325,\n",
        "            \"longitude\": 7.6667\n",
        "        },\n",
        "        \"contacts\": \"non renseigné\",\n",
        "        \"type_de_produits\": \"Produits alimentaires à tarif réduit\",\n",
        "        \"categorie\": \"Epicerie solidaire\",\n",
        "        \"update\": \"2025-01-15\"\n",
        "    },\n",
        "    {\n",
        "        \"url\": \"https://www.francebleu.fr/emissions/l-eco-d-ici-ici-alsace/solibio-la-cooperative-grossiste-alsacienne-bio-a-le-vent-en-poupe-5368146\",\n",
        "        \"description\": \"Solibio, une coopérative grossiste bio en Alsace, en pleine croissance.\",\n",
        "        \"localisation\": {\n",
        "            \"adresse\": \"Marché-Gare, Rue du Marché, Strasbourg, France\",\n",
        "            \"latitude\": 48.584614,\n",
        "            \"longitude\": 7.734722\n",
        "        },\n",
        "        \"contacts\": \"non renseigné\",\n",
        "        \"type_de_produits\": \"Produits bio : fruits, légumes, produits laitiers, viande\",\n",
        "        \"categorie\": \"Coopérative bio\",\n",
        "        \"update\": \"2024-01-15\"\n",
        "    },\n",
        "    {\n",
        "        \"url\": \"https://strasinfo.fr/2025/03/20/le-petit-marche-ouvre-ses-portes-a-cronenbourg-un-nouveau-concept-qui-mele-alimentaire-et-convivialite/\",\n",
        "        \"description\": \"Le Petit Marché : un marché hybride mêlant produits locaux et animations à Cronenbourg.\",\n",
        "        \"localisation\": {\n",
        "            \"adresse\": \"Square Saint-Florent, Cronenbourg, Strasbourg, France\",\n",
        "            \"latitude\": 48.593889,\n",
        "            \"longitude\": 7.709722\n",
        "        },\n",
        "        \"contacts\": \"non renseigné\",\n",
        "        \"type_de_produits\": \"Produits locaux : fruits, légumes, produits laitiers\",\n",
        "        \"categorie\": \"Marché local\",\n",
        "        \"update\": \"2025-03-20\"\n",
        "    },\n",
        "    {\n",
        "        \"url\": \"https://www.vertici.fr\",\n",
        "        \"description\": \"VERT ICI : salades, sandwichs et smoothies personnalisés avec des produits frais.\",\n",
        "        \"localisation\": {\n",
        "            \"adresse\": \"Centre commercial Aubette, Place Kléber, Strasbourg, France\",\n",
        "            \"latitude\": 48.5842,\n",
        "            \"longitude\": 7.7469\n",
        "        },\n",
        "        \"contacts\": \"non renseigné\",\n",
        "        \"type_de_produits\": \"Salades, sandwichs, smoothies\",\n",
        "        \"categorie\": \"Restauration rapide\",\n",
        "        \"update\": \"non renseigné\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm2l1LnXaw1w"
      },
      "source": [
        "# **Sous partie 2** : Amélioration du retour d'information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUaozxPlbH8Y"
      },
      "outputs": [],
      "source": [
        "def predict(question):\n",
        "    results = qa_system.find_best_match(question)\n",
        "    if results:\n",
        "        output = \"\"\n",
        "        for i, result in enumerate(results):\n",
        "            output += f\"**Réponse {i+1}:**\\n\"\n",
        "            output += f\"- **Titre:** {result['titre']}\\n\"\n",
        "            output += f\"- **Description:** {result['description']}\\n\"\n",
        "            output += f\"- **Adresse:** {result['adresse_complete']}\\n\"\n",
        "            output += f\"- **Type:** {result['type']}\\n\"\n",
        "            output += f\"- **Catégorie:** {result['categorie']}\\n\"\n",
        "            if result['url']:\n",
        "                output += f\"- **URL:** {result['url']}\\n\"\n",
        "            output += \"\\n\"\n",
        "        return output\n",
        "    else:\n",
        "        return \"**Aucun résultat pertinent trouvé.**\\n\\nEssayez de reformuler votre question ou d'utiliser des mots-clés différents comme 'panier', 'local', 'bio', 'restaurant', 'épicerie', etc.\"\n",
        "\n",
        "###### Localisation\n",
        "\n",
        "def get_ecological_map_plotly():\n",
        "    \"\"\"Crée une carte Plotly des lieux écologiques.\"\"\"\n",
        "    locations = []\n",
        "    names = []\n",
        "    for item in data:\n",
        "        if \"localisation\" in item and \"latitude\" in item[\"localisation\"] and \"longitude\" in item[\"localisation\"]:\n",
        "            lat = item[\"localisation\"][\"latitude\"]\n",
        "            lon = item[\"localisation\"][\"longitude\"]\n",
        "            name = item.get(\"description\", item[\"localisation\"].get(\"adresse\", \"Lieu\"))\n",
        "            locations.append((lat, lon))\n",
        "            names.append(name)\n",
        "\n",
        "    if not locations:\n",
        "        fig = go.Figure(go.Scattermapbox(lat=[48.584], lon=[7.744], mode='markers', text=[\"Strasbourg (par défaut)\"]))\n",
        "    else:\n",
        "        lats, lons = zip(*locations)\n",
        "        fig = go.Figure(go.Scattermapbox(lat=lats, lon=lons, mode='markers', text=names))\n",
        "\n",
        "    fig.update_layout(\n",
        "        mapbox_style=\"open-street-map\",\n",
        "        mapbox_center={\"lat\": 48.584, \"lon\": 7.744},  # Centre sur Strasbourg par défaut\n",
        "        mapbox_zoom=10,\n",
        "        margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0}\n",
        "    )\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Buvuo5WFHZ0Y"
      },
      "source": [
        "# **Quatrième partie** : l'interface web"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYnPEGNeIBPc"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks(title=\"IA Verte Locale : Strasbourg\") as iface:\n",
        "    with gr.Tab(\"Recherche\"):\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            <div style=\"text-align: center;\">\n",
        "                <h1>IA Verte Locale : Strasbourg</h1>\n",
        "            </div>\n",
        "            <div style=\"display: flex; justify-content: center;\">\n",
        "                <img src=\"https://wehost.fr/wp-content/uploads/2024/04/Strasbourg-1024x683.png\" alt=\"Vue de Strasbourg\" style=\"max-width: 500px; width: 80%;\">\n",
        "            </div>\n",
        "\n",
        "            Posez une question et trouvez les meilleures correspondances dans notre base de données.\n",
        "\n",
        "            **Conseil :** Pour des résultats plus précis, essayez d'inclure des mots-clés comme 'panier', 'local', 'bio', 'restaurant', 'épicerie', etc.\n",
        "            \"\"\"\n",
        "        )\n",
        "        with gr.Row():\n",
        "            question_input = gr.Textbox(label=\"Posez votre question :\")\n",
        "            output_results = gr.Markdown(label=\"Meilleures réponses :\")\n",
        "\n",
        "        question_input.change(fn=predict, inputs=question_input, outputs=output_results)\n",
        "\n",
        "        gr.Markdown(\"## Questions fréquemment posées\")\n",
        "        for question in frequent_questions:\n",
        "            gr.Markdown(f\"- {question}\")\n",
        "\n",
        "    with gr.Tab(\"Jeu du Budget Vert : Week-end Éco-Loisirs\"):\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            ## Planifiez votre week-end éco-responsable à Strasbourg !\n",
        "\n",
        "            Bienvenue dans le jeu du Budget Vert. L'objectif est de planifier un week-end d'activités tout en respectant un budget carbone de **50 points**.\n",
        "\n",
        "            **Choisissez vos activités en cliquant dessus dans la liste ci-dessous.** Les activités sélectionnées apparaîtront dans la section 'Vos choix', et votre budget sera mis à jour en temps réel.\n",
        "\n",
        "            Essayez de choisir des activités qui vous plaisent tout en minimisant votre impact carbone ! Une fois que vous avez terminé, cliquez sur le bouton 'Terminer mon week-end' pour voir votre bilan.\n",
        "            \"\"\"\n",
        "        )\n",
        "        budget_restant = gr.State(BUDGET_INITIAL)\n",
        "        activites_choisies_liste = gr.State([])\n",
        "        activites_choisies_texte = gr.Markdown(\"**Activités choisies :**\")\n",
        "        budget_affichage = gr.Markdown(f\"Budget carbone restant : **{BUDGET_INITIAL}** points\")\n",
        "\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### Liste des activités :\")\n",
        "            activity_buttons = []\n",
        "            for activite_nom, cout in ACTIVITES.items():\n",
        "                button = gr.Button(f\"{activite_nom} ({cout} points)\", variant=\"secondary\")\n",
        "                button.click(\n",
        "                    fn=ajouter_ou_retirer_activite,\n",
        "                    inputs=[gr.Textbox(value=activite_nom, visible=False), budget_restant, activites_choisies_liste, activites_choisies_texte],\n",
        "                    outputs=[budget_restant, activites_choisies_liste, activites_choisies_texte]\n",
        "                )\n",
        "                activity_buttons.append(button)\n",
        "\n",
        "            budget_restant.change(fn=lambda budget: f\"Budget carbone restant : **{budget}** points\", inputs=budget_restant, outputs=budget_affichage)\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"### Vos choix :\")\n",
        "            output_activites_choisies = activites_choisies_texte\n",
        "\n",
        "            terminer_button = gr.Button(\"Terminer mon week-end\", variant=\"primary\")\n",
        "            resultat_jeu = gr.Markdown(visible=False) # Définition ici, avant le bouton \"Terminer\"\n",
        "            terminer_button.click(\n",
        "                fn=terminer_jeu,\n",
        "                inputs=[budget_restant, activites_choisies_liste],\n",
        "                outputs=resultat_jeu\n",
        "            )\n",
        "\n",
        "            resultat_jeu # Affichage du résultat en dernier dans la colonne\n",
        "\n",
        "    with gr.Tab(\"Carte des Lieux Écologiques\"):\n",
        "        plotly_map = gr.Plot(get_ecological_map_plotly())\n",
        "\n",
        "iface.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
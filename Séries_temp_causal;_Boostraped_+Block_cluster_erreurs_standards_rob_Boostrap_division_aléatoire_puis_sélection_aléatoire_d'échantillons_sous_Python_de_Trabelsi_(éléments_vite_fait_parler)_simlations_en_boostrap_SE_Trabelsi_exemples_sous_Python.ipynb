{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlsvSM9tfmvmK6Z75kjO02",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gregroudoudou/M1-Programmation-/blob/Entra%C3%AEnements-python-des-notions-de-cours-Personnels/S%C3%A9ries_temp_causal%3B_Boostraped_%2BBlock_cluster_erreurs_standards_rob_Boostrap_division_al%C3%A9atoire_puis_s%C3%A9lection_al%C3%A9atoire_d'%C3%A9chantillons_sous_Python_de_Trabelsi_(%C3%A9l%C3%A9ments_vite_fait_parler)_simlations_en_boostrap_SE_Trabelsi_exemples_sous_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_bjDsDyipNxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805beab1-1051-473c-c4f7-f7c63b75ede6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moyenne empirique : 3.9200\n",
            "Moyenne bootstrap : 3.9203\n",
            "Intervalle de confiance √† 95% : [3.5100, 4.3200]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Donn√©es\n",
        "data = np.array([2.3, 2.5, 2.7, 2.9, 3.0,\n",
        "                 3.2, 3.3, 3.4, 3.8, 4.1,\n",
        "                 4.2, 4.3, 4.3, 4.4, 4.5,\n",
        "                 4.7, 4.9, 5.1, 5.3, 5.5])\n",
        "\n",
        "# 2. Fonction de bootstrap\n",
        "def bootstrap_mean(data, n_bootstrap=10000):\n",
        "    \"\"\"\n",
        "    data: array-like\n",
        "        Donn√©es initiales\n",
        "    n_bootstrap: int\n",
        "        Nombre de r√©pliques bootstrap\n",
        "    \"\"\"\n",
        "    n = len(data)\n",
        "    means = np.empty(n_bootstrap)  # pour stocker les moyennes\n",
        "\n",
        "    for i in range(n_bootstrap):\n",
        "        sample = np.random.choice(data, size=n, replace=True)\n",
        "        means[i] = np.mean(sample)\n",
        "\n",
        "    return means\n",
        "\n",
        "# 3. Ex√©cuter la simulation\n",
        "bootstrap_results = bootstrap_mean(data, n_bootstrap=10000)\n",
        "\n",
        "# 4. Analyse\n",
        "moy_empirique = np.mean(data)\n",
        "print(f\"Moyenne empirique : {moy_empirique:.4f}\")\n",
        "\n",
        "moy_bootstrap = np.mean(bootstrap_results)\n",
        "print(f\"Moyenne bootstrap : {moy_bootstrap:.4f}\")\n",
        "\n",
        "ic_95_inf = np.percentile(bootstrap_results, 2.5)\n",
        "ic_95_sup = np.percentile(bootstrap_results, 97.5)\n",
        "print(f\"Intervalle de confiance √† 95% : [{ic_95_inf:.4f}, {ic_95_sup:.4f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "n (ou parfois len(data) : correspond g√©n√©ralement au nombre d‚Äôobservations dans l‚Äô√©chantillon d‚Äôorigine.\n",
        "n_bootstrap : correspond au nombre de r√©pliques (c‚Äôest-√†-dire le nombre de fois qu‚Äôon va tirer un nouvel √©chantillon de taille n avec remise)."
      ],
      "metadata": {
        "id": "qQcdM22QLRHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans un essai clinique, on compare deux traitements diff√©rents (A et B) pour voir lequel est le plus efficace. Vous avez deux √©chantillons de tailles diff√©rentes, contenant les mesures d‚Äôefficacit√© (par exemple, tension art√©rielle avant/apr√®s traitement ou niveau de cholest√©rol r√©duit, etc.)."
      ],
      "metadata": {
        "id": "DH1O2F4TMoR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comment le bootstrap intervient ?\n",
        "√âchantillon A : donn√©es du groupe A, de taille n‚ÇÅ.\n",
        "√âchantillon B : donn√©es du groupe B, de taille n‚ÇÇ.\n",
        "Vous estimez la diff√©rence de moyennes (ex.\n",
        "ùëã\n",
        "Àâ\n",
        "ùê¥\n",
        "‚àí\n",
        "ùëã\n",
        "Àâ\n",
        "ùêµ\n",
        "X\n",
        "Àâ\n",
        "  \n",
        "A\n",
        "‚Äã\n",
        " ‚àí\n",
        "X\n",
        "Àâ\n",
        "  \n",
        "B\n",
        "‚Äã\n",
        " ).\n",
        "Pour obtenir un intervalle de confiance de cette diff√©rence, vous g√©n√©rez des r√©pliques bootstrap s√©par√©ment pour A et pour B (√† chaque r√©plique, vous tirez avec remise dans chaque groupe).\n",
        "Vous calculez la diff√©rence de moyennes pour chaque r√©plique.\n",
        "Vous examinez la distribution de ces diff√©rences pour en extraire un IC (par exemple, un IC √† 95 % via les quantiles 2.5 % et 97.5 %)."
      ],
      "metadata": {
        "id": "Bs_0LxE5MsfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Avoir  une validation d'un mod√®le en machine learning : tester l'√©cart en termes de R2 ou de score F1....etc !\n"
      ],
      "metadata": {
        "id": "Bojn4JsENBJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1. G√©n√©ration d'un dataset fictif (binaire) avec 1000 √©chantillons\n",
        "# ---------------------------------------------------------------------\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_redundant=2,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2. S√©paration en jeu d'entra√Ænement et de test\n",
        "# ---------------------------------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3. Entra√Ænement d'un mod√®le (Logistic Regression)\n",
        "# ---------------------------------------------------------------------\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4. √âvaluation sur l'ensemble de test (score \"brut\")\n",
        "# ---------------------------------------------------------------------\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy sur le jeu de test : {accuracy_test:.3f}\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5. M√©thode du bootstrap pour estimer l'incertitude\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# Indices du jeu de test (on va piocher avec remise l√†-dedans)\n",
        "test_indices = np.arange(len(X_test))\n",
        "\n",
        "# Nombre de r√©√©chantillonnages bootstrap\n",
        "n_bootstrap = 1000\n",
        "\n",
        "# Pour stocker la pr√©cision calcul√©e √† chaque r√©plique\n",
        "bootstrap_accuracies = np.empty(n_bootstrap)\n",
        "\n",
        "# Boucle de r√©√©chantillonnage\n",
        "for i in range(n_bootstrap):\n",
        "    # Tirage avec remise parmi les indices du test\n",
        "    sample_indices = np.random.choice(test_indices,\n",
        "                                      size=len(test_indices),\n",
        "                                      replace=True)\n",
        "\n",
        "    # On r√©cup√®re X et y correspondant aux indices choisis : =les m√©langes de plusiuers ann√©es...!/indices d'√©chantillons :\n",
        "    X_sample = X_test[sample_indices]\n",
        "    y_sample = y_test[sample_indices]\n",
        "\n",
        "    # On calcule la pr√©diction du m√™me mod√®le sur cet √©chantillon bootstrap\n",
        "    y_pred_sample = model.predict(X_sample)\n",
        "\n",
        "    # On mesure la pr√©cision pour tout les modeles:\n",
        "    bootstrap_accuracies[i] = accuracy_score(y_sample, y_pred_sample)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6. Analyse des r√©sultats bootstrap\n",
        "# ---------------------------------------------------------------------\n",
        "accuracy_mean = np.mean(bootstrap_accuracies)\n",
        "accuracy_std = np.std(bootstrap_accuracies, ddof=1)\n",
        "\n",
        "# Intervalle de confiance √† 95% (m√©thode percentiles)\n",
        "ci_lower = np.percentile(bootstrap_accuracies, 2.5)\n",
        "ci_upper = np.percentile(bootstrap_accuracies, 97.5)\n",
        "\n",
        "print(f\"Accuracy moyenne (bootstrap) : {accuracy_mean:.3f}\")\n",
        "print(f\"Ecart-type (bootstrap)       : {accuracy_std:.3f}\")\n",
        "print(f\"IC 95% (bootstrap)           : [{ci_lower:.3f}, {ci_upper:.3f}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SnCHnwyNAX6",
        "outputId": "345dd4a8-1e05-4b42-cd5d-2606e0967924"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy sur le jeu de test : 0.823\n",
            "Accuracy moyenne (bootstrap) : 0.823\n",
            "Ecart-type (bootstrap)       : 0.023\n",
            "IC 95% (bootstrap)           : [0.780, 0.867]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quand vous avez un jeu de donn√©es (par exemple, un ensemble de test ou un ensemble de validation) et que vous voulez conna√Ætre la pr√©cision ou le score F1 d‚Äôun mod√®le, vous pouvez :\n",
        "\n",
        "Cr√©er plusieurs √©chantillons bootstrap du jeu de test (toujours avec remise).\n",
        "√âvaluer votre mod√®le sur chacun de ces √©chantillons.\n",
        "Analyser la distribution des performances mesur√©es : moyenne, √©cart-type, quantiles.\n",
        "Cela vous donne une id√©e de la variabilit√© de la mesure de performance. Vous pouvez par exemple dire"
      ],
      "metadata": {
        "id": "MCjUDk4_NK0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1. G√©n√©ration d'un dataset fictif (binaire) avec 1000 √©chantillons\n",
        "# ---------------------------------------------------------------------\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_redundant=2,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2. S√©paration en jeu d'entra√Ænement et de test\n",
        "# ---------------------------------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3. Entra√Ænement d'un mod√®le (Logistic Regression)\n",
        "# ---------------------------------------------------------------------\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4. √âvaluation sur l'ensemble de test (score \"brut\")\n",
        "# ---------------------------------------------------------------------\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy sur le jeu de test : {accuracy_test:.3f}\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5. M√©thode du bootstrap pour estimer l'incertitude\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# Indices du jeu de test (on va piocher avec remise l√†-dedans)\n",
        "test_indices = np.arange(len(X_test))\n",
        "\n",
        "# Nombre de r√©√©chantillonnages bootstrap\n",
        "n_bootstrap = 1000\n",
        "\n",
        "# Pour stocker la pr√©cision calcul√©e √† chaque r√©plique\n",
        "bootstrap_accuracies = np.empty(n_bootstrap)\n",
        "\n",
        "# Boucle de r√©√©chantillonnage\n",
        "for i in range(n_bootstrap):\n",
        "    # Tirage avec remise parmi les indices du test\n",
        "    sample_indices = np.random.choice(test_indices,\n",
        "                                      size=len(test_indices),\n",
        "                                      replace=True)\n",
        "\n",
        "    # On r√©cup√®re X et y correspondant aux indices choisis\n",
        "    X_sample = X_test[sample_indices]\n",
        "    y_sample = y_test[sample_indices]\n",
        "\n",
        "    # On calcule la pr√©diction du m√™me mod√®le sur cet √©chantillon bootstrap\n",
        "    y_pred_sample = model.predict(X_sample)\n",
        "\n",
        "    # On mesure la pr√©cision\n",
        "    bootstrap_accuracies[i] = accuracy_score(y_sample, y_pred_sample)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6. Analyse des r√©sultats bootstrap\n",
        "# ---------------------------------------------------------------------\n",
        "accuracy_mean = np.mean(bootstrap_accuracies)\n",
        "accuracy_std = np.std(bootstrap_accuracies, ddof=1)\n",
        "\n",
        "# Intervalle de confiance √† 95% (m√©thode percentiles)\n",
        "ci_lower = np.percentile(bootstrap_accuracies, 2.5)\n",
        "ci_upper = np.percentile(bootstrap_accuracies, 97.5)\n",
        "\n",
        "print(f\"Accuracy moyenne (bootstrap) : {accuracy_mean:.3f}\")\n",
        "print(f\"Ecart-type (bootstrap)       : {accuracy_std:.3f}\")\n",
        "print(f\"IC 95% (bootstrap)           : [{ci_lower:.3f}, {ci_upper:.3f}]\")\n"
      ],
      "metadata": {
        "id": "gFkPrwQCOdna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Exemples de comparaison random forest avec le reste , notamment en partant"
      ],
      "metadata": {
        "id": "isdDighTKuTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JWEYuZMYPk7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.BOOstrap sur les indices des \"folds\": c√†dire qu'on en r√©plique certain, d√®s fois , d√®s fois pas afin de tester la STABILITE de l'INDICATEUR !\n"
      ],
      "metadata": {
        "id": "H_kPlcuXQBbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1. G√©n√©ration d'un dataset fictif (binaire) avec 1000 √©chantillons\n",
        "# ---------------------------------------------------------------------\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_redundant=2,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2. S√©paration en jeu d'entra√Ænement et de test\n",
        "# ---------------------------------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3. Entra√Ænement de deux mod√®les :\n",
        "#    - Logistic Regression\n",
        "#    - Random Forest\n",
        "# ---------------------------------------------------------------------\n",
        "model_lr = LogisticRegression()\n",
        "model_lr.fit(X_train, y_train)\n",
        "\n",
        "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_rf.fit(X_train, y_train)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4. √âvaluation initiale sur l'ensemble de test (score \"brut\")\n",
        "# ---------------------------------------------------------------------\n",
        "y_pred_lr = model_lr.predict(X_test)\n",
        "y_pred_rf = model_rf.predict(X_test)\n",
        "\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "diff_accuracy = accuracy_rf - accuracy_lr\n",
        "\n",
        "print(\"=== Accuracy sur le jeu de test ===\")\n",
        "print(f\"  - Logistic Regression : {accuracy_lr:.3f}\")\n",
        "print(f\"  - Random Forest      : {accuracy_rf:.3f}\")\n",
        "print(f\"  - Diff√©rence (RF - LR) : {diff_accuracy:.3f}\\n\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5. M√©thode du bootstrap pour comparer les deux mod√®les\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# Indices du jeu de test (on va piocher avec remise l√†-dedans)\n",
        "test_indices = np.arange(len(X_test))\n",
        "\n",
        "# Nombre de r√©√©chantillonnages bootstrap\n",
        "n_bootstrap = 2000\n",
        "\n",
        "# Pour stocker la pr√©cision calcul√©e √† chaque r√©plique\n",
        "bootstrap_acc_lr = np.empty(n_bootstrap)\n",
        "bootstrap_acc_rf = np.empty(n_bootstrap)\n",
        "bootstrap_diff   = np.empty(n_bootstrap)\n",
        "\n",
        "# Boucle de r√©√©chantillonnage\n",
        "for i in range(n_bootstrap):\n",
        "    # Tirage avec remise parmi les indices du test\n",
        "    sample_indices = np.random.choice(test_indices,\n",
        "                                      size=len(test_indices),\n",
        "                                      replace=True)\n",
        "\n",
        "    # On r√©cup√®re X et y correspondant aux indices choisis\n",
        "    X_sample = X_test[sample_indices]\n",
        "    y_sample = y_test[sample_indices]\n",
        "\n",
        "    # On calcule les pr√©dictions de chaque mod√®le\n",
        "    y_pred_sample_lr = model_lr.predict(X_sample)\n",
        "    y_pred_sample_rf = model_rf.predict(X_sample)\n",
        "\n",
        "    # On mesure l'accuracy de chaque mod√®le\n",
        "    acc_lr = accuracy_score(y_sample, y_pred_sample_lr)\n",
        "    acc_rf = accuracy_score(y_sample, y_pred_sample_rf)\n",
        "\n",
        "    # On stocke\n",
        "    bootstrap_acc_lr[i] = acc_lr\n",
        "    bootstrap_acc_rf[i] = acc_rf\n",
        "    bootstrap_diff[i]   = acc_rf - acc_lr\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6. Analyse des r√©sultats bootstrap\n",
        "# ---------------------------------------------------------------------\n",
        "acc_lr_mean = np.mean(bootstrap_acc_lr)\n",
        "acc_lr_std  = np.std(bootstrap_acc_lr, ddof=1)\n",
        "ci_lr_lower = np.percentile(bootstrap_acc_lr, 2.5)\n",
        "ci_lr_upper = np.percentile(bootstrap_acc_lr, 97.5)\n",
        "\n",
        "acc_rf_mean = np.mean(bootstrap_acc_rf)\n",
        "acc_rf_std  = np.std(bootstrap_acc_rf, ddof=1)\n",
        "ci_rf_lower = np.percentile(bootstrap_acc_rf, 2.5)\n",
        "ci_rf_upper = np.percentile(bootstrap_acc_rf, 97.5)\n",
        "\n",
        "diff_mean = np.mean(bootstrap_diff)\n",
        "diff_std  = np.std(bootstrap_diff, ddof=1)\n",
        "ci_diff_lower = np.percentile(bootstrap_diff, 2.5)\n",
        "ci_diff_upper = np.percentile(bootstrap_diff, 97.5)\n",
        "\n",
        "print(\"=== R√©sultats bootstrap ===\")\n",
        "print(\"--> Logistic Regression :\")\n",
        "print(f\"  - Moyenne accuracy : {acc_lr_mean:.3f}\")\n",
        "print(f\"  - Ecart-type       : {acc_lr_std:.3f}\")\n",
        "print(f\"  - IC 95%           : [{ci_lr_lower:.3f}, {ci_lr_upper:.3f}]\\n\")\n",
        "\n",
        "print(\"--> Random Forest :\")\n",
        "print(f\"  - Moyenne accuracy : {acc_rf_mean:.3f}\")\n",
        "print(f\"  - Ecart-type       : {acc_rf_std:.3f}\")\n",
        "print(f\"  - IC 95%           : [{ci_rf_lower:.3f}, {ci_rf_upper:.3f}]\\n\")\n",
        "\n",
        "print(\"--> Diff√©rence (RF - LR) :\")\n",
        "print(f\"  - Moyenne diff : {diff_mean:.3f}\")\n",
        "print(f\"  - Ecart-type   : {diff_std:.3f}\")\n",
        "print(f\"  - IC 95%       : [{ci_diff_lower:.3f}, {ci_diff_upper:.3f}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPqZXKM0QHDd",
        "outputId": "60021efb-3ab8-4a7c-dd53-99a051049e28"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Accuracy sur le jeu de test ===\n",
            "  - Logistic Regression : 0.823\n",
            "  - Random Forest      : 0.940\n",
            "  - Diff√©rence (RF - LR) : 0.117\n",
            "\n",
            "=== R√©sultats bootstrap ===\n",
            "--> Logistic Regression :\n",
            "  - Moyenne accuracy : 0.823\n",
            "  - Ecart-type       : 0.022\n",
            "  - IC 95%           : [0.780, 0.867]\n",
            "\n",
            "--> Random Forest :\n",
            "  - Moyenne accuracy : 0.940\n",
            "  - Ecart-type       : 0.014\n",
            "  - IC 95%           : [0.913, 0.967]\n",
            "\n",
            "--> Diff√©rence (RF - LR) :\n",
            "  - Moyenne diff : 0.117\n",
            "  - Ecart-type   : 0.023\n",
            "  - IC 95%       : [0.070, 0.163]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Estimer les METRIQUES : avec random forest : !"
      ],
      "metadata": {
        "id": "ofiRgN47RHfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1. G√©n√©ration d'un dataset factice (classification binaire)\n",
        "# ---------------------------------------------------------------------\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_redundant=2,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2. Configuration du Stratified K-Fold\n",
        "#    - n_splits=5 => 5 sous-√©chantillons\n",
        "#    - shuffle=True + random_state=42 => reproductibilit√© des tirages\n",
        "# ---------------------------------------------------------------------\n",
        "skf = StratifiedKFold(n_splits=15, shuffle=True, random_state=42)\n",
        "\n",
        "# Pour stocker les mesures de performance √† chaque split\n",
        "scores = []\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3. Boucle sur les splits\n",
        "# ---------------------------------------------------------------------\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    # Cr√©ation des ensembles d'entra√Ænement et de test\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Instanciation du Random Forest\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=100,   # nombre d'arbres\n",
        "        max_depth=None,     # profondeur max (None => pas de limite)\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Entra√Ænement sur le fold courant\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Pr√©diction sur l'ensemble de test du fold\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calcul de l'accuracy pour ce fold\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    scores.append(acc)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4. R√©sultats finaux\n",
        "# ---------------------------------------------------------------------\n",
        "mean_score = np.mean(scores)\n",
        "std_score = np.std(scores, ddof=1)\n",
        "\n",
        "print(f\"Scores de pr√©cision (accuracy) par fold : {scores}\")\n",
        "print(f\"Moyenne des scores : {mean_score:.3f}\")\n",
        "print(f\"Ecart-type         : {std_score:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzsAuKB3RlB4",
        "outputId": "214993f4-1e77-4205-fc66-32be2ab2dee1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores de pr√©cision (accuracy) par fold : [0.9402985074626866, 0.8955223880597015, 0.8805970149253731, 0.9552238805970149, 0.9552238805970149, 0.9701492537313433, 0.8955223880597015, 0.9552238805970149, 0.9104477611940298, 0.9701492537313433, 0.9242424242424242, 0.8787878787878788, 0.9545454545454546, 0.9393939393939394, 0.8939393939393939]\n",
            "Moyenne des scores : 0.928\n",
            "Ecart-type         : 0.033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stratified_bootstrap(X, y, n_samples=None):\n",
        "    \"\"\"\n",
        "    X : features\n",
        "    y : labels (classes)\n",
        "    n_samples : taille de l'√©chantillon final √† tirer (par d√©faut len(X))\n",
        "\n",
        "    Retourne (X_boot, y_boot), un bootstrap stratifi√© de X, y\n",
        "    \"\"\"\n",
        "    if n_samples is None:\n",
        "        n_samples = len(X)\n",
        "\n",
        "    # S√©paration par classes\n",
        "    unique_classes, counts = np.unique(y, return_counts=True)\n",
        "    proportions = counts / len(y)\n",
        "\n",
        "    # On stocke ici les √©chantillons\n",
        "    X_boot = []\n",
        "    y_boot = []\n",
        "\n",
        "    for cls, prop in zip(unique_classes, proportions):\n",
        "        # on filtre pour prendre que les √©l√©ments de cette classe\n",
        "        X_cls = X[y == cls]\n",
        "        y_cls = y[y == cls]\n",
        "\n",
        "        # nb d'√©l√©ments √† piocher pour cette classe,\n",
        "        # sur la base des proportions * n_samples\n",
        "        n_cls_samples = int(round(prop * n_samples))\n",
        "\n",
        "        # tirage bootstrap avec remise\n",
        "        idx_choice = np.random.choice(\n",
        "            np.arange(len(X_cls)),\n",
        "            size=n_cls_samples,\n",
        "            replace=True\n",
        "        )\n",
        "\n",
        "        # ajout aux listes\n",
        "        X_boot.append(X_cls[idx_choice])\n",
        "        y_boot.append(y_cls[idx_choice])\n",
        "\n",
        "    # concat√©ner (attention √† la forme, si c'est un ndarray ou DataFrame)\n",
        "    X_boot = np.concatenate(X_boot)\n",
        "    y_boot = np.concatenate(y_boot)\n",
        "\n",
        "    return X_boot, y_boot\n"
      ],
      "metadata": {
        "id": "qOujuxpXS_am"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Savoir cr√©er un boostrap sur les coeffcieints d'une OLS: v√©rifier la stabilit√© d'une r√©gression !"
      ],
      "metadata": {
        "id": "GQAQNVduVqLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 1. G√©n√©ration des donn√©es\n",
        "# -----------------------------\n",
        "np.random.seed(42)\n",
        "n = 200\n",
        "X = np.random.uniform(0, 10, n)             # X ~ U(0,10)\n",
        "beta_0, beta_1 = 2.0, 1.5                  # vrais param√®tres\n",
        "epsilon = np.random.normal(0, 2, n)        # bruit ~ N(0, 2^2)\n",
        "\n",
        "y = beta_0 + beta_1 * X + epsilon\n",
        "\n",
        "# On pr√©pare X pour statsmodels (ajout de la constante)\n",
        "X_const = sm.add_constant(X)  # shape = (n,2) => [ [1, X1], [1, X2], ...]\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Estimation OLS sur l'√©chantillon initial\n",
        "# -----------------------------\n",
        "model_ols = sm.OLS(y, X_const).fit()\n",
        "print(\"=== OLS classique ===\")\n",
        "print(model_ols.summary())\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Bootstrap non param√©trique\n",
        "# -----------------------------\n",
        "n_boot = 1000\n",
        "coef_boot = np.empty(n_boot)\n",
        "\n",
        "for i in range(n_boot):\n",
        "    # Tirage d'indices avec remise\n",
        "    idx = np.random.choice(np.arange(n), size=n, replace=True)\n",
        "    X_boot = X_const[idx, :]\n",
        "    y_boot = y[idx]\n",
        "\n",
        "    # Estimation OLS sur l'√©chantillon bootstrap√©\n",
        "    model_bs = sm.OLS(y_boot, X_boot).fit()\n",
        "    coef_boot[i] = model_bs.params[1]  # on stocke le coefficient associ√© √† X\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Intervalle de confiance empirique pour beta_1\n",
        "# -----------------------------\n",
        "beta1_lower = np.percentile(coef_boot, 2.5)\n",
        "beta1_upper = np.percentile(coef_boot, 97.5)\n",
        "\n",
        "print(\"\\n=== R√©sultats Bootstrap (coefficient sur X) ===\")\n",
        "print(f\"Moyenne bootstrap : {np.mean(coef_boot):.3f}\")\n",
        "print(f\"IC 95% bootstrap  : [{beta1_lower:.3f}, {beta1_upper:.3f}]\")\n",
        "\n",
        "# (Optionnel) Visualisation de la distribution bootstrap\n",
        "plt.figure()\n",
        "plt.hist(coef_boot, bins=30)\n",
        "plt.axvline(beta1_lower, color='r', linestyle='--')\n",
        "plt.axvline(beta1_upper, color='r', linestyle='--')\n",
        "plt.title(\"Distribution bootstrap du coefficient beta_1\")\n",
        "plt.xlabel(\"Valeur du coefficient\")\n",
        "plt.ylabel(\"Fr√©quence\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "id": "9LmltII5VuJm",
        "outputId": "17b5892e-1483-4895-b6a8-b1d765c353b7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== OLS classique ===\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   R-squared:                       0.836\n",
            "Model:                            OLS   Adj. R-squared:                  0.835\n",
            "Method:                 Least Squares   F-statistic:                     1011.\n",
            "Date:                Tue, 11 Mar 2025   Prob (F-statistic):           1.04e-79\n",
            "Time:                        17:53:24   Log-Likelihood:                -415.57\n",
            "No. Observations:                 200   AIC:                             835.1\n",
            "Df Residuals:                     198   BIC:                             841.7\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          2.2104      0.264      8.358      0.000       1.689       2.732\n",
            "x1             1.4844      0.047     31.790      0.000       1.392       1.576\n",
            "==============================================================================\n",
            "Omnibus:                        7.028   Durbin-Watson:                   2.131\n",
            "Prob(Omnibus):                  0.030   Jarque-Bera (JB):                9.199\n",
            "Skew:                           0.231   Prob(JB):                       0.0101\n",
            "Kurtosis:                       3.943   Cond. No.                         11.2\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "\n",
            "=== R√©sultats Bootstrap (coefficient sur X) ===\n",
            "Moyenne bootstrap : 1.483\n",
            "IC 95% bootstrap  : [1.391, 1.580]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAHHCAYAAACvJxw8AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAS3ZJREFUeJzt3Xd4FNX+x/HPpoeQhJpGSSLSqxSRJghRBOQCoiCCBIzKVZCqCBaqioIKoiB6rwSxg/2KIAgoRUTpgkgTKQKhSWISkkAyvz/yy8KSZNg0Nsm8X88zD7NnZme+e3b25MuZMzM2wzAMAQAAIEdurg4AAACgOCNZAgAAMEGyBAAAYIJkCQAAwATJEgAAgAmSJQAAABMkSwAAACZIlgAAAEyQLAEAAJggWUKhmTRpkmw22zXZV4cOHdShQwf76++//142m02ffPLJNdn/oEGDFBERcU32dblr/TmtzmazadKkSa4Oo8Di4uJ01113qWLFirLZbJo1a5Ykad++fbrtttsUGBgom82mL774QgsWLJDNZtOff/6Zp3246jdhZtCgQSpbtqyrw0ApQLKEHGU1mFmTj4+PwsLC1LlzZ82ePVv//PNPoezn2LFjmjRpkrZt21Yo2ytMxTk2V/jxxx81adIknTt3Ls/vpS5da9SoUfr22281fvx4vfvuu7r99tslSdHR0fr111/13HPP6d1331Xz5s1dHKm53377TZMmTcpzIleY5s6dqwULFhTpPn7++Wc98sgjatasmTw9Pa/Zf0JhwgByEBsba0gypkyZYrz77rvG/Pnzjeeff9647bbbDJvNZoSHhxvbt293eM+FCxeM8+fP52k/v/zyiyHJiI2NzdP7UlNTjdTUVPvr1atXG5KMxYsX52k7+Y0tLS3NSElJKbR9OasoPqezZsyYYUgyDh48mOf35vd7djVJxsSJE10dRoEFBwcb/fv3dyhLTk42JBlPPfWUQ/nFixeN8+fPGxkZGXnax7X4TSxevNiQZKxevdqp9aOjow0/P79CjaF+/fpG+/btC3WbV5o4caLh6elpNGvWzKhVq5bBn2rXo2cJprp06aIBAwZo8ODBGj9+vL799lt99913OnnypP71r3/p/Pnz9nU9PDzk4+NTpPEkJydLkry8vOTl5VWk+zLj6ekpb29vl+2/tMv6nlE4Tp48qXLlyjmUnTp1SpKylbu7u8vHxyfPvRn8JgrPww8/rPj4eG3atEm33nqrq8OBOA2HfOjYsaOeeeYZHTp0SO+99569PKcxSytWrFDbtm1Vrlw5lS1bVrVr19aTTz4pKXP8TYsWLSRJgwcPtp/yy+ri7tChgxo0aKDNmzfr5ptvVpkyZezvvXLMUpb09HQ9+eSTCgkJkZ+fn/71r3/pyJEjDutERERo0KBB2d57+TavFltO4zOSkpI0ZswYVatWTd7e3qpdu7ZeeuklGYbhsJ7NZtOwYcP0xRdfqEGDBvL29lb9+vW1bNmynCs8B858TklavHixmjVrJl9fX1WqVEkDBgzQX3/9lW29VatWqV27dvLz81O5cuXUo0cP7d6927580qRJevzxxyVJkZGR9vrIOh1SVN/zl19+qW7duiksLEze3t6qUaOGpk6dqvT0dIf4L99G69at5evrq8jISM2bN8+p+kxNTdWoUaNUuXJl+fv761//+peOHj2abb3cxuXkZbzexo0b1bVrV5UvX15+fn5q1KiRXn31VYd1rvZ9ZPnrr790//33Kzg42H4czZ8/374863S6YRiaM2eOve4nTZqk8PBwSdLjjz8um81m/1y5jVlaunSp2rdvL39/fwUEBKhFixb64IMPTOsmIyNDs2bNUv369eXj46Pg4GANGTJEf//9t8N6ERERuuOOO7Ru3TrdeOON8vHx0XXXXaeFCxc6fJa7775bknTLLbfYP8v3339/1Tr/448/1LlzZ/n5+SksLExTpkzJ9rt0JtaIiAjt2rVLP/zwg33/WW3G2bNn9dhjj6lhw4YqW7asAgIC1KVLF23fvv2q8V0pODhYvr6+eX4fio6HqwNAyXTffffpySef1PLly/Xggw/muM6uXbt0xx13qFGjRpoyZYq8vb21f/9+rV+/XpJUt25dTZkyRRMmTNBDDz2kdu3aSZJat25t38aZM2fUpUsX3XPPPRowYICCg4NN43ruuedks9n0xBNP6OTJk5o1a5aioqK0bdu2PDU+zsR2OcMw9K9//UurV69WTEyMmjRpom+//VaPP/64/vrrL82cOdNh/XXr1umzzz7TI488In9/f82ePVu9e/fW4cOHVbFixavG58znXLBggQYPHqwWLVpo2rRpiouL06uvvqr169dr69at9h6F7777Tl26dNF1112nSZMm6fz583rttdfUpk0bbdmyRREREbrzzju1d+9effjhh5o5c6YqVaokSapcuXKRfs8LFixQ2bJlNXr0aJUtW1arVq3ShAkTlJCQoBkzZjjUyd9//62uXbuqT58+6tevnxYtWqSHH35YXl5euv/++03r84EHHtB7772ne++9V61bt9aqVavUrVu3q34PebVixQrdcccdCg0N1YgRIxQSEqLdu3fr66+/1ogRIyQ5931ImYO2b7rpJnvyXblyZS1dulQxMTFKSEjQyJEjdfPNN+vdd9/Vfffdp1tvvVUDBw6UJDVq1EjlypXTqFGj1K9fP3Xt2tV0IPSCBQt0//33q379+ho/frzKlSunrVu3atmyZbr33ntzfd+QIUPsx+Hw4cN18OBBvf7669q6davWr18vT09P+7r79+/XXXfdpZiYGEVHR2v+/PkaNGiQmjVrpvr16+vmm2/W8OHDNXv2bD355JOqW7euJNn/zU16erpuv/123XTTTZo+fbqWLVumiRMn6uLFi5oyZUqeYp01a5YeffRRlS1bVk899ZQk2Y/VP/74Q1988YXuvvtuRUZGKi4uTm+++abat2+v3377TWFhYaZxophz7VlAFFdZY5Z++eWXXNcJDAw0brjhBvvriRMnOpxbnzlzpiHJOHXqVK7bMBvL0r59e0OSMW/evByXXT5uIGssT5UqVYyEhAR7+aJFiwxJxquvvmovCw8PN6Kjo6+6TbPYoqOjjfDwcPvrL774wpBkPPvssw7r3XXXXYbNZjP2799vL5NkeHl5OZRt377dkGS89tpr2fZ1OWc/Z1pamhEUFGQ0aNDAYRzZ119/bUgyJkyYYC9r0qSJERQUZJw5c8YhHjc3N2PgwIH2stzGLBXl95ycnJytbMiQIUaZMmUcxsdkbePll1+2l6Wmpto/W1paWq6xbdu2zZBkPPLIIw7l9957b7YxS1d+71muPPZzcvHiRSMyMtIIDw83/v77b4dll48Pcvb7iImJMUJDQ43Tp087bOuee+4xAgMDHepOkjF06FCH9Q4ePGhIMmbMmOFQnvXbz/qez507Z/j7+xstW7bMNibx8rivrJu1a9cakoz333/f4T3Lli3LVh4eHm5IMtasWWMvO3nypOHt7W2MGTPGXpafMUuSjEcffdQh5m7duhleXl72YzYvseY2ZiklJcVIT093KDt48KDh7e1tTJkyxal4czJ06FDGLBUDnIZDvpUtW9b0qrisnosvv/xSGRkZ+dqHt7e3Bg8e7PT6AwcOlL+/v/31XXfdpdDQUH3zzTf52r+zvvnmG7m7u2v48OEO5WPGjJFhGFq6dKlDeVRUlGrUqGF/3ahRIwUEBOiPP/5wan9X+5ybNm3SyZMn9cgjjziMI+vWrZvq1KmjJUuWSJKOHz+ubdu2adCgQapQoYJDPLfeeqtT9VaU3/PlvYH//POPTp8+rXbt2ik5OVm///67w7oeHh4aMmSI/bWXl5eGDBmikydPavPmzbnuO+szXvndjRw5Mj8fJVdbt27VwYMHNXLkyGzjhLJO4Tn7fRiGoU8//VTdu3eXYRg6ffq0fercubPi4+O1ZcuWQol7xYoV+ueffzRu3LhsYxLNTj0uXrxYgYGBuvXWWx3ia9asmcqWLavVq1c7rF+vXj17r6OU2WtZu3Ztp38TZoYNG+YQ87Bhw5SWlqbvvvsuX7HmxNvbW25umX9S09PTdebMGfsp6cL6LuA6JEvIt8TERIc/2Ffq27ev2rRpowceeEDBwcG65557tGjRojz9Qa1SpUqeBnLXrFnT4bXNZtP1119f5JcaHzp0SGFhYdnqI+sUwaFDhxzKq1evnm0b5cuXzzaWIzdX+5xZ+6tdu3a299apU8e+3Gy9unXr6vTp00pKSjKNpSi/5127dqlXr14KDAxUQECAKleurAEDBkiS4uPjHdYNCwuTn5+fQ1mtWrUkyfT7P3TokNzc3BySVynnOimIAwcOSJIaNGhgGktu+778+zh16pTOnTunt956S5UrV3aYspLOkydPXrO4c7Jv3z7Fx8crKCgoW4yJiYnZ4ivobyI3bm5uuu666xzKrjwu8hprTjIyMjRz5kzVrFlT3t7eqlSpkipXrqwdO3ZkO1ZR8jBmCfly9OhRxcfH6/rrr891HV9fX61Zs0arV6/WkiVLtGzZMn388cfq2LGjli9fLnd396vupygGOeb2v+H09HSnYioMue3HuGLQaUlQVN/zuXPn1L59ewUEBGjKlCmqUaOGfHx8tGXLFj3xxBP57sUqCLNj51rK+uwDBgxQdHR0jus0atToWoaUTUZGhoKCgvT+++/nuLxy5coOr135m8hrrDl5/vnn9cwzz+j+++/X1KlTVaFCBbm5uWnkyJEuOVZRuEiWkC/vvvuuJKlz586m67m5ualTp07q1KmTXnnlFT3//PN66qmntHr1akVFRRX6zdb27dvn8NowDO3fv9/hD0f58uVzvLHioUOHHP4HmpfYwsPD9d133+mff/5x6F3KOlWUdeVRYbna58za3549e9SxY0eHdffs2WNffvl6V/r9999VqVIle2+NWX0Uxff8/fff68yZM/rss890880328sPHjyY4/rHjh1TUlKSQ+/S3r17Jcn0ztLh4eHKyMjQgQMHHHp0cqoTs2PnarJ6rnbu3KmoqKhcY8lt35d/Hz4+PvL391d6enqu2yosl8dt9p+jnN733XffqU2bNoX2n578HEcZGRn6448/7L1JUvbjIi+x5hbDJ598oltuuUVvv/22Q/m5c+fsF0Sg5OI0HPJs1apVmjp1qiIjI9W/f/9c1zt79my2siZNmkjKvFRbkv0PW37uCp2ThQsXOoyj+uSTT3T8+HF16dLFXlajRg399NNPSktLs5d9/fXX2S69z0tsXbt2VXp6ul5//XWH8pkzZ8pmsznsvzBc7XM2b95cQUFBmjdvnr2upczLv3fv3m2/0is0NFRNmjTRO++84/A5d+7cqeXLl6tr1672stzqo6i+56yehst7FtLS0jR37twc17948aLefPNNh3XffPNNVa5cWc2aNct1P1l1Nnv2bIfyrEeCXK5GjRqKj4/Xjh077GXHjx/X559/ftXP07RpU0VGRmrWrFnZ6iHrMzr7fbi7u6t379769NNPtXPnzmz7yrqHUmG47bbb5O/vr2nTpiklJSXHuHPSp08fpaena+rUqdmWXbx4MV+/+fy2F5f/Lg3D0Ouvvy5PT0916tQpz7H6+fnluH93d/ds9bF48eIcb9WBkoeeJZhaunSpfv/9d128eFFxcXFatWqVVqxYofDwcH311VemN6GcMmWK1qxZo27duik8PFwnT57U3LlzVbVqVbVt21ZS5h+fcuXKad68efL395efn59atmypyMjIfMVboUIFtW3bVoMHD1ZcXJxmzZql66+/3uH2Bg888IA++eQT3X777erTp48OHDig9957L9uYlbzE1r17d91yyy166qmn9Oeff6px48Zavny5vvzyS40cOTLbtgvqap/T09NTL774ogYPHqz27durX79+9lsHREREaNSoUfZtzZgxQ126dFGrVq0UExNjv1Q9MDDQ4bloWQnHU089pXvuuUeenp7q3r17kX3PrVu3Vvny5RUdHa3hw4fLZrPp3XffzfUPdFhYmF588UX9+eefqlWrlj7++GNt27ZNb731lsMl6ldq0qSJ+vXrp7lz5yo+Pl6tW7fWypUrtX///mzr3nPPPXriiSfUq1cvDR8+XMnJyXrjjTdUq1atqw7idXNz0xtvvKHu3burSZMmGjx4sEJDQ/X7779r165d+vbbb/P0fbzwwgtavXq1WrZsqQcffFD16tXT2bNntWXLFn333Xc5JrH5ERAQoJkzZ+qBBx5QixYtdO+996p8+fLavn27kpOT9c477+T4vvbt22vIkCGaNm2atm3bpttuu02enp7at2+fFi9erFdffVV33XVXnmJp0qSJ3N3d9eKLLyo+Pl7e3t7q2LGjgoKCcn2Pj4+Pli1bpujoaLVs2VJLly7VkiVL9OSTT9pPr+Ul1mbNmumNN97Qs88+q+uvv15BQUHq2LGj7rjjDk2ZMkWDBw9W69at9euvv+r999/PNl7KGYcOHbL33m/atEmS9Oyzz0rK7H2877778rxNFJBLrsFDsZd1+XDW5OXlZYSEhBi33nqr8eqrrzpctp7lysunV65cafTo0cMICwszvLy8jLCwMKNfv37G3r17Hd735ZdfGvXq1TM8PDwcLi9v3769Ub9+/Rzjy+3WAR9++KExfvx4IygoyPD19TW6detmHDp0KNv7X375ZaNKlSqGt7e30aZNG2PTpk3ZtmkWW06XkP/zzz/GqFGjjLCwMMPT09OoWbOmMWPGjGyPjVAOl3EbRu63NLhcXj/nxx9/bNxwww2Gt7e3UaFCBaN///7G0aNHs6333XffGW3atDF8fX2NgIAAo3v37sZvv/2Wbb2pU6caVapUMdzc3OyXlxfl97x+/XrjpptuMnx9fY2wsDBj7Nixxrfffpvt8vGsbWzatMlo1aqV4ePjY4SHhxuvv/66aX1mOX/+vDF8+HCjYsWKhp+fn9G9e3fjyJEjOT7uZPny5UaDBg0MLy8vo3bt2sZ7773n1K0Dsqxbt8649dZbDX9/f8PPz89o1KhRtltGOPt9xMXFGUOHDjWqVatmeHp6GiEhIUanTp2Mt956y2G9nI45Z28dkOWrr74yWrdubY/pxhtvND788EP78txuq/DWW28ZzZo1M3x9fQ1/f3+jYcOGxtixY41jx47Z1wkPDze6deuW7b05/Sb/85//GNddd53h7u5+1dsIZD3u5MCBA8Ztt91mlClTxggODjYmTpyY7TJ/Z2M9ceKE0a1bN8Pf39+QZI8vJSXFGDNmjBEaGmr4+voabdq0MTZs2JDjZ7iarN95TlNRP2oFObMZRgkcUQoAl+nQoYNOnz6d4ykpACgoxiwBAACYYMwSAADXwKlTp0xvM+Hl5eVwM1IUHyRLAABcAy1atDC9zUT79u2dejAwrj3GLAEAcA2sX79e58+fz3V5+fLlTW9zAdchWQIAADDBAG8AAAATjFlS5u3wjx07Jn9//0J//AYAACgahmHon3/+UVhYmNzciq7/h2RJmc+UqlatmqvDAAAA+XDkyBFVrVq1yLZPsiTZH3x65MgRBQQEuDgaFKmkJCksLHP+2DHpsoeuAkCuaDuKpYSEBFWrVs3hAeZFgWRJl54iHRAQQLJU2v3/w1klSQEBNHgAnEPbUawV9RAaBngDAACYIFkCAAAwwWk4WIuHhxQdfWkeAJxB22FpLu9ZWrNmjbp3766wsDDZbDZ98cUXDssNw9CECRMUGhoqX19fRUVFad++fQ7rnD17Vv3791dAQIDKlSunmJgYJSYmXsNPgRLD21tasCBz8vZ2dTQASgraDktzebKUlJSkxo0ba86cOTkunz59umbPnq158+Zp48aN8vPzU+fOnZWSkmJfp3///tq1a5dWrFihr7/+WmvWrNFDDz10rT4CAAAoxYrV405sNps+//xz9ezZU1Jmr1JYWJjGjBmjxx57TJIUHx+v4OBgLViwQPfcc492796tevXq6ZdfflHz5s0lScuWLVPXrl119OhRhWVd6mkiISFBgYGBio+P52q40s4wpOTkzPkyZSRuQgrAGbQdxdK1+vvt8p4lMwcPHtSJEycUFRVlLwsMDFTLli21YcMGSdKGDRtUrlw5e6IkSVFRUXJzc9PGjRuvecwo5pKTpbJlM6eshg8Aroa2w9KK9Si1EydOSJKCg4MdyoODg+3LTpw4oaCgIIflHh4eqlChgn2dK6Wmpio1NdX+OiEhoTDDBgAApUix7lkqKtOmTVNgYKB94lEnAAAgN8U6WQoJCZEkxcXFOZTHxcXZl4WEhOjkyZMOyy9evKizZ8/a17nS+PHjFR8fb5+OHDlSBNEDAIDSoFgnS5GRkQoJCdHKlSvtZQkJCdq4caNatWolSWrVqpXOnTunzZs329dZtWqVMjIy1LJlyxy36+3tbX+0CY84AQAAZlw+ZikxMVH79++3vz548KC2bdumChUqqHr16ho5cqSeffZZ1axZU5GRkXrmmWcUFhZmv2Kubt26uv322/Xggw9q3rx5unDhgoYNG6Z77rnHqSvhAAAAzLg8Wdq0aZNuueUW++vRo0dLkqKjo7VgwQKNHTtWSUlJeuihh3Tu3Dm1bdtWy5Ytk4+Pj/0977//voYNG6ZOnTrJzc1NvXv31uzZs6/5ZwEAAKVPsbrPkqtwnyULSUmR7rsvc/7dd6XLkm4AyBVtR7F0rf5+kyyJZAkAgJLoWv39dvlpOAAoahHjluT7vX++0K0QIwFQEhXrq+EAAABcjWQJ1pKUlPlMJ5stcx4AnEHbYWkkSwAAACZIlgAAAEyQLAEAAJggWQIAADBBsgQAAGCCZAkAAMAEN6WEtbi7S127XpoHAGfQdlgayRKsxcdHWpL/uzkDsCjaDkvjNBwAAIAJkiUAAAATJEuwlqQkyc8vc+KRBQCcRdthaYxZgvUkJ7s6AgAlEW2HZdGzBAAAYIJkCQAAwATJEgAAgAmSJQAAABMkSwAAACa4Gg7W4uYmtW9/aR4AnEHbYWkkS7AWX1/p++9dHQWAkoa2w9JIjwEAAEyQLAEAAJggWYK1JCVJlStnTjyyAICzaDssjTFLsJ7Tp10dAYCSiLbDsuhZAgAAMEGyBAAAYIJkCQAAwATJEgAAgAmSJQAAABNcDQdrcXOTmje/NA8AzqDtsDSSJViLr6/0yy+ujgJASUPbYWmkxwAAACZIlgAAAEyQLMFakpOliIjMKTnZ1dEAKCloOyyNMUuwFsOQDh26NA8AzqDtsDR6lgAAAEyQLAEAAJggWQIAADBBsgQAAGCCZAkAAMAEV8PBWmw2qV69S/MA4AzaDksjWYK1lCkj7drl6igAlDS0HZbGaTgAAAATJEsAAAAmOA0Ha0lOllq0yJz/5ZfMrnWUCBHjlrg6BFgZbYelkSzBWgxD+u23S/MA4AzaDkvjNBwAAIAJkiUAAAATJEsAAAAmSJYAAABMkCwBAACY4Go4WIvNJoWHX5oHAGfQdlgayRKspUwZ6c8/XR0FgJKGtsPSOA0HAABggmQJAADABMkSrOX8+cxHFrRokTkPAM6g7bA0xizBWjIypE2bLs0DgDNoOyyNniUAAAATxT5ZSk9P1zPPPKPIyEj5+vqqRo0amjp1qozLHmRoGIYmTJig0NBQ+fr6KioqSvv27XNh1AAAoLQo9snSiy++qDfeeEOvv/66du/erRdffFHTp0/Xa6+9Zl9n+vTpmj17tubNm6eNGzfKz89PnTt3VkpKigsjBwAApUGxH7P0448/qkePHurWrZskKSIiQh9++KF+/vlnSZm9SrNmzdLTTz+tHj16SJIWLlyo4OBgffHFF7rnnntcFjsAACj5in3PUuvWrbVy5Urt3btXkrR9+3atW7dOXbp0kSQdPHhQJ06cUFRUlP09gYGBatmypTZs2JDjNlNTU5WQkOAwAQAA5KTY9yyNGzdOCQkJqlOnjtzd3ZWenq7nnntO/fv3lySdOHFCkhQcHOzwvuDgYPuyK02bNk2TJ08u2sBRfFWq5OoILCti3BJXhwDkH22HZRX7nqVFixbp/fff1wcffKAtW7bonXfe0UsvvaR33nkn39scP3684uPj7dORI0cKMWIUa35+0qlTmZOfn6ujAVBS0HZYWrHvWXr88cc1btw4+9ijhg0b6tChQ5o2bZqio6MVEhIiSYqLi1NoaKj9fXFxcWrSpEmO2/T29pa3t3eRxw4AAEq+Yt+zlJycLDc3xzDd3d2V8f83BYuMjFRISIhWrlxpX56QkKCNGzeqVatW1zRWAABQ+hT7nqXu3bvrueeeU/Xq1VW/fn1t3bpVr7zyiu6//35Jks1m08iRI/Xss8+qZs2aioyM1DPPPKOwsDD17NnTtcGj+Dl/Xvr/iwO0dKnk6+vaeFCqFWSM1p8vdCvESFBgtB2WVuyTpddee03PPPOMHnnkEZ08eVJhYWEaMmSIJkyYYF9n7NixSkpK0kMPPaRz586pbdu2WrZsmXx8fFwYOYqljAzphx8uzQOAM2g7LM1mXH4rbItKSEhQYGCg4uPjFRAQ4OpwUJSSkqSyZTPnExMZqHmNlcSr4QrSw0PPUilC21EsXau/38V+zBIAAIArFfvTcADgSiWxNwxA4aJnCQAAwATJEgAAgAlOw8F6ypRxdQQASiLaDssiWYK1+PllXtUCAHlB22FpnIYDAAAwQbIEAABggmQJ1pKSInXrljmlpLg6GgAlBW2HpTFmCdaSni59882leQBwBm2HpdGzBAAAYIJkCQAAwATJEgAAgAmSJQAAABMkSwAAACZIlgAAAExw6wBYi5+fZBiujgJASUPbYWn0LAEAAJggWQIAADBBsgRrSUmR7r47c+KRBQCcRdthaSRLsJb0dOmTTzInHlkAwFm0HZZGsgQAAGCCZAkAAMAEyRIAAIAJkiUAAAATJEsAAAAmSJYAAABM8LgTWEuZMlJi4qV5AHAGbYelkSzBWmy2zGc8AUBe0HZYGqfhAAAATJAswVpSU6VBgzKn1FRXRwOgpKDtsDSSJVjLxYvSO+9kThcvujoaACUFbYelkSwBAACYYIA3gDyJGLfE1SEAwDVFzxIAAIAJkiUAAAATJEsAAAAmSJYAAABMMMAb1lKmjHTy5KV5AHAGbYelkSzBWmw2qXJlV0cBoKSh7bA0kiXAgrj8HwCcx5glWEtqqjR0aObEIwsAOIu2w9JIlmAtFy9Kc+dmTjyyAICzaDssjWQJAADABMkSAACACZIlAAAAEyRLAAAAJrh1AAAUQwW5vcOfL3QrxEgA0LMEAABggp4lWIuvr3Tw4KV5AHAGbYelkSzBWtzcpIgIV0cBoKSh7bA0TsMBAACYIFmCtaSlSY8/njmlpbk6GgAlBW2HpZEswVouXJBeeilzunDB1dEAKCloOyyNZAkAAMAEyRIAAIAJkiUAAAATJEsAAAAmSJYAAABMkCwBAACY4A7esBZfX2nnzkvzAOAM2g5LKxE9S3/99ZcGDBigihUrytfXVw0bNtSmTZvsyw3D0IQJExQaGipfX19FRUVp3759LowYxZabm1S/fubkViIOfwDFAW2HpRX7b/zvv/9WmzZt5OnpqaVLl+q3337Tyy+/rPLly9vXmT59umbPnq158+Zp48aN8vPzU+fOnZWSkuLCyAEAQGlQ4NNw+/fv14EDB3TzzTfL19dXhmHIZrMVRmySpBdffFHVqlVTbGysvSwyMtI+bxiGZs2apaefflo9evSQJC1cuFDBwcH64osvdM899xRaLCgF0tKk55/PnH/yScnLy7XxACgZaDssLd89S2fOnFFUVJRq1aqlrl276vjx45KkmJgYjRkzptAC/Oqrr9S8eXPdfffdCgoK0g033KD//Oc/9uUHDx7UiRMnFBUVZS8LDAxUy5YttWHDhhy3mZqaqoSEBIcJFnHhgjR5cubEIwsAOIu2w9LynSyNGjVKHh4eOnz4sMqUKWMv79u3r5YtW1YowUnSH3/8oTfeeEM1a9bUt99+q4cffljDhw/XO++8I0k6ceKEJCk4ONjhfcHBwfZlV5o2bZoCAwPtU7Vq1QotXgAAULrk+zTc8uXL9e2336pq1aoO5TVr1tShQ4cKHFiWjIwMNW/eXM//f/fnDTfcoJ07d2revHmKjo7O1zbHjx+v0aNH218nJCSQMAEAgBzlu2cpKSnJoUcpy9mzZ+Xt7V2goC4XGhqqevXqOZTVrVtXhw8fliSFhIRIkuLi4hzWiYuLsy+7kre3twICAhwmAACAnOQ7WWrXrp0WLlxof22z2ZSRkaHp06frlltuKZTgJKlNmzbas2ePQ9nevXsVHh4uKXOwd0hIiFauXGlfnpCQoI0bN6pVq1aFFgcAALCmfJ+Gmz59ujp16qRNmzYpLS1NY8eO1a5du3T27FmtX7++0AIcNWqUWrdureeff159+vTRzz//rLfeektvvfWWpMwkbeTIkXr22WdVs2ZNRUZG6plnnlFYWJh69uxZaHEAAABryney1KBBA+3du1evv/66/P39lZiYqDvvvFNDhw5VaGhooQXYokULff755xo/frymTJmiyMhIzZo1S/3797evM3bsWCUlJemhhx7SuXPn1LZtWy1btkw+Pj6FFgcAALAmm2EYhquDcLWEhAQFBgYqPj6e8UulXXq6tGVL5nzTppK7u2vjcZGIcUtcHQKK0J8vdHN1CKUPbUexdK3+fue7Zyk2NlZly5bV3Xff7VC+ePFiJScn5/tKNaBIubtLLVq4OgoAJQ1th6Xle4D3tGnTVKlSpWzlQUFB9sv8AQAASrp89ywdPnzY4bEjWcLDw+2X9QPFTlqa9OqrmfMjRvDIAgDOoe2wtHz3LAUFBWnHjh3Zyrdv366KFSsWKCigyFy4II0dmznxyAIAzqLtsLR8J0v9+vXT8OHDtXr1aqWnpys9PV2rVq3SiBEjeHgtAAAoNfJ9Gm7q1Kn6888/1alTJ3l4ZG4mIyNDAwcOZMwSAAAoNfKdLHl5eenjjz/W1KlTtX37dvn6+qphw4b2O2sDAACUBvlOlrLUqlVLtWrVKoxYAAAAip18J0vp6elasGCBVq5cqZMnTyojI8Nh+apVqwocHAAAgKvlO1kaMWKEFixYoG7duqlBgway2WyFGRcAAECxkO9k6aOPPtKiRYvUtWvXwowHKFo+PtLq1ZfmAcAZtB2WVqAB3tdff31hxgIUPXd3qUMHV0cBFFsFeW5gqX4mHW2HpeX7PktjxozRq6++Kp7DCwAASrN89yytW7dOq1ev1tKlS1W/fn15eno6LP/ss88KHBxQ6C5ckN56K3P+oYekK45bAMgRbYel5TtZKleunHr16lWYsQBFLy1NGjYsc37QIBo8AM6h7bC0fCdLsbGxhRkHAABAsZTvMUuSdPHiRX333Xd688039c8//0iSjh07psTExEIJDgAAwNXy3bN06NAh3X777Tp8+LBSU1N16623yt/fXy+++KJSU1M1b968wowTAADAJQp0U8rmzZtr+/btqlixor28V69eevDBBwslOABA3hXk8n8A2eU7WVq7dq1+/PFHeXl5OZRHRETor7/+KnBgAAAAxUG+xyxlZGQoPT09W/nRo0fl7+9foKAAAACKi3wnS7fddptmzZplf22z2ZSYmKiJEyfyCBQUX97e0tdfZ07e3q6OBkBJQdthafk+Dffyyy+rc+fOqlevnlJSUnTvvfdq3759qlSpkj788MPCjBEoPB4eUrdS/EgGAEWDtsPS8p0sVa1aVdu3b9dHH32kHTt2KDExUTExMerfv798fX0LM0YAAACXyXeyJEkeHh4aMGBAYcUCFL0LF6T338+c79+fu/ACcA5th6XlO1lauHCh6fKBAwfmd9NA0UlLkwYPzpy/+24aPADOoe2wtALdZ+lyFy5cUHJysry8vFSmTBmSJQAAUCrk+2q4v//+22FKTEzUnj171LZtWwZ4AwCAUqNAz4a7Us2aNfXCCy9k63UCAAAoqQo0wDvHDXp46NixY4W9WQBX4JEWAHBt5DtZ+uqrrxxeG4ah48eP6/XXX1ebNm0KHBgAAEBxkO9kqWfPng6vbTabKleurI4dO+rll18uaFwAAADFQr6TpYyMjMKMA7g2vL2lRYsuzQOAM2g7LK3QxywBxZqHR+Y9UgAgL2g7LC3fydLo0aOdXveVV17J724AAABcKt/J0tatW7V161ZduHBBtWvXliTt3btX7u7uatq0qX09m81W8CiBwnLxovT555nzvXpl/m8RAK6GtsPS8v1td+/eXf7+/nrnnXdUvnx5SZk3qhw8eLDatWunMWPGFFqQQKFJTZX69MmcT0ykwQPgHNoOS8v3TSlffvllTZs2zZ4oSVL58uX17LPPcjUcAAAoNfKdLCUkJOjUqVPZyk+dOqV//vmnQEEBAAAUF/lOlnr16qXBgwfrs88+09GjR3X06FF9+umniomJ0Z133lmYMQIAALhMvk+6zps3T4899pjuvfdeXbhwIXNjHh6KiYnRjBkzCi1AAAAAV8p3slSmTBnNnTtXM2bM0IEDByRJNWrUkJ+fX6EFBwAA4Gr5Pg2X5fjx4zp+/Lhq1qwpPz8/GYZRGHEBAAAUC073LGVkZMjN7VJudebMGfXp00erV6+WzWbTvn37dN111ykmJkbly5fnijgUT15eUmzspXkAcAZth6U53bP0yiuv6JtvvrG/HjVqlDw9PXX48GGVKVPGXt63b18tW7ascKMECounpzRoUObk6enqaACUFLQdluZ0z9Ktt96q3r176/jx44qJidHy5cv17bffqmrVqg7r1axZU4cOHSr0QAEAAFzB6Z6lxo0b6+eff9YXX3whSUpKSnLoUcpy9uxZefNEZhRXFy9KS5ZkThcvujoaACUFbYel5WmAd4UKFfS///1PktSuXTstXLjQvsxmsykjI0PTp0/XLbfcUrhRAoUlNVW6447MKTXV1dEAKCloOywt37cOmD59ujp16qRNmzYpLS1NY8eO1a5du3T27FmtX7++MGMEAABwmXzfOqBBgwbau3ev2rZtqx49eigpKUl33nmntm7dqho1ahRmjAAAAC6Tr56lCxcu6Pbbb9e8efP01FNPFXZMAAAAxUa+epY8PT21Y8eOwo4FAACg2Mn3abgBAwbo7bffLsxYAAAAip18D/C+ePGi5s+fr++++07NmjXL9ky4V155pcDBAQAAuFqek6U//vhDERER2rlzp5o2bSpJ2rt3r8M6NputcKIDCpuXl/T665fmAcAZtB2WludkqWbNmjp+/LhWr14tKfPxJrNnz1ZwcHChBwcUOk9PaehQV0cBoKSh7bC0PI9ZMgzD4fXSpUuVlJRUaAEBAAAUJ/kes5TlyuQJKNbS06W1azPn27WT3N1dGw+AkoG2w9LynCzZbLZsY5IYo4QSIyVFynocT2KidMWFCQCQI9oOS8tzsmQYhgYNGmR/WG5KSor+/e9/Z7sa7rPPPiucCAEAAFwoz8lSdHS0w+sBAwYUWjAAAADFTZ6TpdjY2KKIw2kvvPCCxo8frxEjRmjWrFmSMnu3xowZo48++kipqanq3Lmz5s6dyxV6AACgwPJ9B29X+OWXX/Tmm2+qUaNGDuWjRo3S//73Py1evFg//PCDjh07pjvvvNNFUQIAgNKkxCRLiYmJ6t+/v/7zn/+ofPny9vL4+Hi9/fbbeuWVV9SxY0c1a9ZMsbGx+vHHH/XTTz+5MGIAAFAalJhkaejQoerWrZuioqIcyjdv3qwLFy44lNepU0fVq1fXhg0bctxWamqqEhISHCYAAICcFPg+S9fCRx99pC1btuiXX37JtuzEiRPy8vJSuXLlHMqDg4N14sSJHLc3bdo0TZ48uShCRXHn6SlNn35pHgCcQdthacU+WTpy5IhGjBihFStWyMfHp1C2OX78eI0ePdr+OiEhQdWqVSuUbaOY8/KSHn/c1VEAKGloOyyt2J+G27x5s06ePKmmTZvKw8NDHh4e+uGHHzR79mx5eHgoODhYaWlpOnfunMP74uLiFBISkuM2vb29FRAQ4DABAADkpNj3LHXq1Em//vqrQ9ngwYNVp04dPfHEE6pWrZo8PT21cuVK9e7dW5K0Z88eHT58WK1atXJFyCjO0tOlLVsy55s25ZEFAJxD22FpxT5Z8vf3V4MGDRzK/Pz8VLFiRXt5TEyMRo8erQoVKiggIECPPvqoWrVqpZtuuskVIaM4S0mRbrwxc55HFgCFKmLckny/988XuhViJEWAtsPSin2y5IyZM2fKzc1NvXv3drgpJQAAQEGVyGTp+++/d3jt4+OjOXPmaM6cOa4JCAAAlFrFfoA3AACAK5EsAQAAmCBZAgAAMEGyBAAAYKJEDvAG8s3TU5o48dI8ADiDtsPSSJZgLV5e0qRJro4CQElD22FpnIYDAAAwQc8SrCUjQ9q9O3O+bl3Jjf8vAHACbYelkSzBWs6fl7Ien8MjCwA4i7bD0kiNAQAATJAsAQAAmCBZAgAAMEGyBAAAYIJkCQAAwATJEgAAgAluHQBr8fSUHnvs0ryLRYxb4uoQADijmLUduLZIlmAtXl7SjBmujgJASUPbYWmchgMAADBBzxKsJSNDOnw4c756dR5ZAMA5tB2WRrIEazl/XoqMzJznkQUAnEXbYWmkxgAAACZIlgAAAEyQLAEAAJggWQIAADBBsgQAAGCCZAkAAMAEtw6AtXh4SI88cmkeAJxB22FpfOOwFm9vac4cV0cBoKSh7bA0TsMBAACYoGcJ1mIY0unTmfOVKkk2m2vjAVAy0HZYGskSrCU5WQoKypznkQUAnEXbYWmchgMAADBBsgQAAGCCZAkAAMAEyRIAAIAJkiUAAAATJEsAAAAmuHUArMXDQ4qOvjQPAM6g7bA0vnFYi7e3tGCBq6MAUNLQdlgap+EAAABM0LMEazGMzDvxSlKZMjyyAIBzaDssjZ4lWEtyslS2bOaU1fABwNXQdlgayRIAAIAJkiUAAAATJEsAAAAmSJYAAABMkCwBAACYIFkCAAAwwX2WYC3u7tJdd12aBwBn0HZYGskSrMXHR1q82NVRAChpaDssjdNwAAAAJuhZAgooYtwSV4cAlHgF+R39+UK3QowEyI6eJVhLUlLmM51stsx5AHAGbYelkSwBAACYIFkCAAAwQbIEAABggmQJAADABMkSAACACZIlAAAAE9xnCdbi7i517XppHgCcQdthaSRLsBYfH2kJN5EEkEe0HZZW7E/DTZs2TS1atJC/v7+CgoLUs2dP7dmzx2GdlJQUDR06VBUrVlTZsmXVu3dvxcXFuShiAABQmhT7ZOmHH37Q0KFD9dNPP2nFihW6cOGCbrvtNiVddgfVUaNG6X//+58WL16sH374QceOHdOdd97pwqgBAEBpUexPwy1btszh9YIFCxQUFKTNmzfr5ptvVnx8vN5++2198MEH6tixoyQpNjZWdevW1U8//aSbbrrJFWGjuEpKkoKCMudPnpT8/FwbD4CSgbbD0op9z9KV4uPjJUkVKlSQJG3evFkXLlxQVFSUfZ06deqoevXq2rBhQ47bSE1NVUJCgsMEC0lOzpwAIC9oOyyrRCVLGRkZGjlypNq0aaMGDRpIkk6cOCEvLy+VK1fOYd3g4GCdOHEix+1MmzZNgYGB9qlatWpFHToAACihSlSyNHToUO3cuVMfffRRgbYzfvx4xcfH26cjR44UUoQAAKC0KfZjlrIMGzZMX3/9tdasWaOqVavay0NCQpSWlqZz58459C7FxcUpJCQkx215e3vL29u7qEMGAAClQLHvWTIMQ8OGDdPnn3+uVatWKTIy0mF5s2bN5OnpqZUrV9rL9uzZo8OHD6tVq1bXOlwAAFDKFPuepaFDh+qDDz7Ql19+KX9/f/s4pMDAQPn6+iowMFAxMTEaPXq0KlSooICAAD366KNq1aoVV8IBAIACK/bJ0htvvCFJ6tChg0N5bGysBg0aJEmaOXOm3Nzc1Lt3b6Wmpqpz586aO3fuNY4UJYKbm9S+/aV5AHAGbYel2QzDMFwdhKslJCQoMDBQ8fHxCggIcHU4KGEixvEIBMCV/nyhm6tDgItcq7/fpMcAAAAmSJYAAABMkCzBWpKSpMqVM6fLni8IAKZoOyyt2A/wBgrd6dOujgBASUTbYVkkSwCAEq0gF1kwOBzO4DQcAACACZIlAAAAEyRLAAAAJkiWAAAATDDAG9bi5iY1b35pHgCcQdthaSRLsBZfX+mXX1wdBYCShrbD0kiWAPF8NwBA7uhLBAAAMEGyBGtJTpYiIjKn5GRXRwOgpKDtsDROw8FaDEM6dOjSPABLc/YUvG9ainb/f9tR9+mlOu/lw92/LYSeJQAAABMkSwAAACZIlgAAAEyQLAEAAJggWQIAADDB1XCwFptNqlfv0jwAOMGwSXsrVrfPw1pIlmAtZcpIu3a5OgoAJUyKp49ue2Cuq8OAi3AaDgAAwATJEgAAgAmSJVhLcrJUv37mxCMLADjJ50KKlv/3ES3/7yPyuZDi6nBwjTFmCdZiGNJvv12aBwAn2Ayp1pnD9nlYCz1LAAAAJkiWAAAATJAsAQAAmGDMEkqNiHFLrrqOb1qKdv//fN1nlum8l0/RBgUAKPHoWQIAADBBzxIsxbBJRwOC7PMA4AzaDmsjWUKx4syptIJI8fRR24fnF+k+AJQ+tB3Wxmk4AAAAEyRLAAAAJkiWYCneF1L15Tuj9OU7o+R9IdXV4QAoIWg7rI0xS7AUN8NQ4xP77PMA4Iyc2o6iHmNp5s8Xurls31ZEzxIAAIAJkiUAAAATJEsAAAAmGLOEQufK8/gAABQ2epYAAABM0LMEyznjG+DqEACUQLQd1kWyBEs57+WjZsM/cHUYAEoY2g5r4zQcAACACZIlAAAAEyRLsBTvC6n66INx+uiDcTyyAIDTaDusjTFLsBQ3w9BNR3ba5wHAGbQd1kbPEgAAgAmSJQAAABOchgMAAE4pyBMa/nyhWyFGcm3RswQAAGCCZAkAAMAEp+FgOcme3q4OAUAJRNthXSRLsJTzXj6qN/pTV4cBoISh7bA2TsMBAACYIFkCAAAwwWk4WIr3xTS98fnzkqSHez2pVA8vF0cEoCSg7bA2kqVizlX3tCjIfoszt4wMdfxjk30eAJxRmtqO0tq+FyVOwwEAAJgoVcnSnDlzFBERIR8fH7Vs2VI///yzq0MCAAAlXKk5Dffxxx9r9OjRmjdvnlq2bKlZs2apc+fO2rNnj4KCglwaG12eAIDCxN+Va6vU9Cy98sorevDBBzV48GDVq1dP8+bNU5kyZTR//nxXhwYAAEqwUpEspaWlafPmzYqKirKXubm5KSoqShs2bHBhZAAAoKQrFafhTp8+rfT0dAUHBzuUBwcH6/fff8+2fmpqqlJTU+2v4+PjJUkJCQlFEl9GanKRbPdqCvJ5XBVzUUtPS1FWraSnJivDKNlXtQC4Nmg7Cq4o/sZmbdMwjELf9uVKRbKUV9OmTdPkyZOzlVerVs0F0RSdwFmujqB4CsyamTvQlWEAKGFoOwqmKP8mnTlzRoGBgVdfMZ9KRbJUqVIlubu7Ky4uzqE8Li5OISEh2dYfP368Ro8ebX+dkZGhs2fPqmLFirLZbEUeb3GVkJCgatWq6ciRIwoICHB1OMUKdWOO+jFH/ZijfnJH3ZiLj49X9erVVaFChSLdT6lIlry8vNSsWTOtXLlSPXv2lJSZAK1cuVLDhg3Ltr63t7e8vR2fHl2uXLlrEGnJEBAQwI8yF9SNOerHHPVjjvrJHXVjzs2taIdgl4pkSZJGjx6t6OhoNW/eXDfeeKNmzZqlpKQkDR482NWhAQCAEqzUJEt9+/bVqVOnNGHCBJ04cUJNmjTRsmXLsg36BgAAyItSkyxJ0rBhw3I87QbneHt7a+LEidlOUYK6uRrqxxz1Y476yR11Y+5a1Y/NKOrr7QAAAEqwUnFTSgAAgKJCsgQAAGCCZAkAAMAEyRIAAIAJkqVSas2aNerevbvCwsJks9n0xRdfmK6/bt06tWnTRhUrVpSvr6/q1KmjmTNnOqwzadIk2Ww2h6lOnTpF+CmKRl7r5nLr16+Xh4eHmjRpkm3ZnDlzFBERIR8fH7Vs2VI///xz4QV9DRVF/ZSWY0fKe/18//332T67zWbTiRMnHNaz6vHjTP2UluMnP7+t1NRUPfXUUwoPD5e3t7ciIiI0f/58h3UWL16sOnXqyMfHRw0bNtQ333xTRJ+gaBVF/SxYsCDbsePj45Pn2EiWSqmkpCQ1btxYc+bMcWp9Pz8/DRs2TGvWrNHu3bv19NNP6+mnn9Zbb73lsF79+vV1/Phx+7Ru3bqiCL9I5bVuspw7d04DBw5Up06dsi37+OOPNXr0aE2cOFFbtmxR48aN1blzZ508ebKwwr5miqJ+pNJx7Ej5r589e/Y4fP6goCD7Mo4f8/qRSsfxk5+66dOnj1auXKm3335be/bs0YcffqjatWvbl//444/q16+fYmJitHXrVvXs2VM9e/bUzp07i+IjFKmiqB8p8+7nlx87hw4dyntwBko9Scbnn3+e5/f16tXLGDBggP31xIkTjcaNGxdeYMVAXuqmb9++xtNPP51jPdx4443G0KFD7a/T09ONsLAwY9q0aYUY7bVXWPVTGo8dw3CuflavXm1IMv7+++9c17Hy8eNM/ZTG48eZulm6dKkRGBhonDlzJtd1+vTpY3Tr1s2hrGXLlsaQIUMKI0yXKaz6iY2NNQIDAwscDz1LyNHWrVv1448/qn379g7l+/btU1hYmK677jr1799fhw8fdlGE11ZsbKz++OMPTZw4MduytLQ0bd68WVFRUfYyNzc3RUVFacOGDdcyTJcxq58sVj12sjRp0kShoaG69dZbtX79ens5x0+m3OonixWPn6+++krNmzfX9OnTVaVKFdWqVUuPPfaYzp8/b19nw4YNDseOJHXu3NkSx44z9SNJiYmJCg8PV7Vq1dSjRw/t2rUrz/sqVXfwRsFVrVpVp06d0sWLFzVp0iQ98MAD9mUtW7bUggULVLt2bR0/flyTJ09Wu3bttHPnTvn7+7sw6qK1b98+jRs3TmvXrpWHR/afzOnTp5Wenp7t0TrBwcH6/fffr1WYLnO1+pGse+xIUmhoqObNm6fmzZsrNTVV//3vf9WhQwdt3LhRTZs2tfzxc7X6kax7/Pzxxx9at26dfHx89Pnnn+v06dN65JFHdObMGcXGxkqSTpw4keOxc+WYuNLImfqpXbu25s+fr0aNGik+Pl4vvfSSWrdurV27dqlq1apO74tkCQ7Wrl2rxMRE/fTTTxo3bpyuv/569evXT5LUpUsX+3qNGjVSy5YtFR4erkWLFikmJsZVIRep9PR03XvvvZo8ebJq1arl6nCKHWfrx4rHTpbatWs7jKFo3bq1Dhw4oJkzZ+rdd991YWTFgzP1Y9XjJyMjQzabTe+//74CAwMlSa+88oruuusuzZ07V76+vi6O0LWcqZ9WrVqpVatW9ve0bt1adevW1ZtvvqmpU6c6vS+SJTiIjIyUJDVs2FBxcXGaNGmSPVm6Urly5VSrVi3t37//WoZ4Tf3zzz/atGmTtm7dan/uYEZGhgzDkIeHh5YvX662bdvK3d1dcXFxDu+Ni4tTSEiIK8K+Zpypn44dO2Z7nxWOHTM33nijfYBypUqVLHv85Oby+smJVY6f0NBQValSxZ4ISFLdunVlGIaOHj2qmjVrKiQkxLLHjjP1cyVPT0/dcMMNeT52GLOEXGVkZCg1NTXX5YmJiTpw4IBCQ0OvYVTXVkBAgH799Vdt27bNPv373/9W7dq1tW3bNrVs2VJeXl5q1qyZVq5caX9fRkaGVq5c6fA/mtLImfrJiRWOHTPbtm2zf3YrHz+5ubx+cmKV46dNmzY6duyYEhMT7WV79+6Vm5ub/RRSq1atHI4dSVqxYoUljh1n6udK6enp+vXXX/N87NCzVEolJiY6ZM4HDx7Utm3bVKFCBVWvXl3jx4/XX3/9pYULF0rKvMdL9erV7fcuWbNmjV566SUNHz7cvo3HHntM3bt3V3h4uI4dO6aJEyfK3d09156n4iovdePm5qYGDRo4vD8oKEg+Pj4O5aNHj1Z0dLSaN2+uG2+8UbNmzVJSUpIGDx58zT5XYSmK+iktx46U99/WrFmzFBkZqfr16yslJUX//e9/tWrVKi1fvty+DaseP5Jz9VNajp+81s29996rqVOnavDgwZo8ebJOnz6txx9/XPfff7/9FNyIESPUvn17vfzyy+rWrZs++ugjbdq0KdttX0qCoqifKVOm6KabbtL111+vc+fOacaMGTp06JDDeFynFPh6OhRLWZfjXjlFR0cbhmEY0dHRRvv27e3rz54926hfv75RpkwZIyAgwLjhhhuMuXPnGunp6fZ1+vbta4SGhhpeXl5GlSpVjL59+xr79++/xp+s4PJaN1fK7TLm1157zahevbrh5eVl3HjjjcZPP/1UNB+giBVF/ZSWY8cw8l4/L774olGjRg3Dx8fHqFChgtGhQwdj1apV2bZr1ePHmfopLcdPfn5bu3fvNqKiogxfX1+jatWqxujRo43k5GSHdRYtWmTUqlXL8PLyMurXr28sWbLkGn2iwlUU9TNy5Ej77yo4ONjo2rWrsWXLljzHZjMMw8hbegUAAGAdjFkCAAAwQbIEAABggmQJAADABMkSAACACZIlAAAAEyRLAAAAJkiWAAAATJAsAXDQoUMHjRw50tVh2E2aNElNmjRxdRhXlZycrN69eysgIEA2m03nzp3LsSwiIkKzZs1yapsLFixQuXLlijRuAFdHsgSUEt27d9ftt9+e47K1a9fKZrNpx44d1zgq63jnnXe0du1a/fjjjzp+/LgCAwNzLPvll1/00EMPObXNvn37au/evYUa5/fff29P3AA4h2fDAaVETEyMevfuraNHj2Z7iGRsbKyaN2+uRo0auSg6R2lpafLy8nJ1GIXqwIEDqlu3rsMz8XIqq1y5stPb9PX1tT/jCoDr0LMElBJ33HGHKleurAULFjiUJyYmavHixYqJidGZM2fUr18/ValSRWXKlFHDhg314Ycfmm43NTVVjz32mKpUqSI/Pz+1bNlS33//vX15TqfJZs2apYiICPvrQYMGqWfPnnruuecUFham2rVr57q/F154QcHBwfL391dMTIxSUlIclud0mrBnz54aNGiQ6ef43//+pxYtWsjHx0eVKlVSr1697Mv+/vtvDRw4UOXLl1eZMmXUpUsX7du3z+H969atU7t27eTr66tq1app+PDhSkpKssf08ssva82aNbLZbOrQoUOOZZKynYY7d+6chgwZouDgYPsDiL/++mtJOZ+G+/LLL9W0aVP5+Pjouuuu0+TJk3Xx4kX7cpvNpv/+97/q1auXypQpo5o1a+qrr76SJP3555+65ZZbJEnly5eXzWa7ar0BIFkCSg0PDw8NHDhQCxYs0OWPfFy8eLHS09PVr18/paSkqFmzZlqyZIl27typhx56SPfdd59+/vnnXLc7bNgwbdiwQR999JF27Nihu+++W7fffnu2ZOJqVq5cqT179mjFihX2ZOBKixYt0qRJk/T8889r06ZNCg0N1dy5c/O0n5wsWbJEvXr1UteuXbV161atXLlSN954o335oEGDtGnTJn311VfasGGDDMNQ165ddeHCBUmZPUS33367evfurR07dujjjz/WunXrNGzYMEnSZ599pgcffFCtWrXS8ePH9dlnn+VYdqWMjAx16dJF69ev13vvvafffvtNL7zwgtzd3XP8HGvXrtXAgQM1YsQI/fbbb3rzzTe1YMECPffccw7rTZ48WX369NGOHTvUtWtX9e/fX2fPnlW1atX06aefSpL27Nmj48eP69VXXy1w/QKlXn6fDgyg+Nm9e7chyVi9erW9rF27dsaAAQNyfU+3bt2MMWPG2F+3b9/eGDFihGEYhnHo0CHD3d3d+Ouvvxze06lTJ2P8+PGGYRjGxIkTjcaNGzssnzlzphEeHm5/HR0dbQQHBxupqamm8bdq1cp45JFHHMpatmzpsP3L48vSo0cP+5PJc9tu//79c1y2d+9eQ5Kxfv16e9np06cNX19fY9GiRYZhGEZMTIzx0EMPObxv7dq1hpubm3H+/HnDMAxjxIgR2Z6InlNZeHi4MXPmTMMwDOPbb7813NzcjD179uQYW2xsrBEYGGh/3alTJ+P55593WOfdd981QkND7a8lGU8//bT9dWJioiHJWLp0qWEYl57s/vfff+e4TwDZMWYJKEXq1Kmj1q1ba/78+erQoYP279+vtWvXasqUKZKk9PR0Pf/881q0aJH++usvpaWlKTU1VWXKlMlxe7/++qvS09NVq1Yth/LU1FRVrFgxT7E1bNjwquOUdu/erX//+98OZa1atdLq1avztK8rbdu2TQ8++GCu+/Tw8FDLli3tZRUrVlTt2rW1e/duSdL27du1Y8cOvf/++/Z1DMNQRkaGDh48qLp16+Y7rqpVq2ar39xs375d69evd+hJSk9PV0pKipKTk+3f4+Vj0/z8/BQQEKCTJ0/mK0YADPAGSp2YmBg9+uijmjNnjmJjY1WjRg21b99ekjRjxgy9+uqrmjVrlho2bCg/Pz+NHDlSaWlpOW4rMTFR7u7u2rx5c7ZTQ2XLlpUkubm5OZz2k2Q/fXU5Pz+/wvh4Tu/vcgUdJJ2YmKghQ4Zo+PDh2ZZVr14939vNa1yJiYmaPHmy7rzzzmzLfHx87POenp4Oy2w2mzIyMvIXJADGLAGlTZ8+feTm5qYPPvhACxcu1P333y+bzSZJWr9+vXr06KEBAwaocePGuu6660wvTb/hhhuUnp6ukydP6vrrr3eYQkJCJGVe3XXixAmHBGbbtm35ir1u3brauHGjQ9lPP/3k8Lpy5co6fvy4/XV6erp27txput1GjRpp5cqVue7z4sWLDvs9c+aM9uzZo3r16kmSmjZtqt9++y1bHVx//fUFuqqvUaNGOnr0qNO3B2jatKn27NmTYxxubs4151nxpqen5ztuwGpIloBSpmzZsurbt6/Gjx+v48ePO1ztVLNmTa1YsUI//vijdu/erSFDhiguLi7XbdWqVUv9+/fXwIED9dlnn+ngwYP6+eefNW3aNC1ZskRS5pVgp06d0vTp03XgwAHNmTNHS5cuzVfsI0aM0Pz58xUbG6u9e/dq4sSJ2rVrl8M6HTt21JIlS7RkyRL9/vvvevjhh696z6CJEyfqww8/1MSJE7V79279+uuvevHFF+110qNHDz344INat26dtm/frgEDBqhKlSrq0aOHJOmJJ57Qjz/+qGHDhmnbtm3at2+fvvzyS/sA7/xq3769br75ZvXu3VsrVqzQwYMHtXTpUi1btizH9SdMmKCFCxdq8uTJ2rVrl3bv3q2PPvpITz/9tNP7DA8Pl81m09dff61Tp04pMTGxQJ8BsAKSJaAUiomJ0d9//63OnTsrLCzMXv7000+radOm6ty5szp06KCQkBD17NnTdFuxsbEaOHCgxowZo9q1a6tnz5765Zdf7Kef6tatq7lz52rOnDlq3Lixfv75Zz322GP5irtv37565plnNHbsWDVr1kyHDh3Sww8/7LDO/fffr+joaA0cOFDt27fXddddZ78cPjcdOnTQ4sWL9dVXX6lJkybq2LGjwxWAsbGxatasme644w61atVKhmHom2++sZ/OatSokX744Qft3btX7dq10w033KAJEyY41G1+ffrpp2rRooX69eunevXqaezYsbn2+nTu3Flff/21li9frhYtWuimm27SzJkzFR4e7vT+qlSposmTJ2vcuHEKDg4ucMIHWIHNuPLkPwAAAOzoWQIAADBBsgQAAGCCZAkAAMAEyRIAAIAJkiUAAAATJEsAAAAmSJYAAABMkCwBAACYIFkCAAAwQbIEAABggmQJAADABMkSAACAif8Dcs0N5UVDnxwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Boostrap en s√©ries temporelles : le Block boostrap"
      ],
      "metadata": {
        "id": "RDHGn8e_WFvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "# --------------------------------------\n",
        "# 1. G√©n√©ration d'un petit panel factice\n",
        "# --------------------------------------\n",
        "N = 10   # nombre d'individus\n",
        "T = 8    # nombre de p√©riodes\n",
        "total_obs = N * T\n",
        "\n",
        "# Identifiants individuels\n",
        "ids = np.repeat(np.arange(N), T)\n",
        "# P√©riodes\n",
        "time = np.tile(np.arange(T), N)\n",
        "\n",
        "# Variables explicatives\n",
        "X1 = np.random.normal(size=total_obs)\n",
        "X2 = np.random.normal(size=total_obs)\n",
        "\n",
        "# Effet fixe par individu (par ex.)\n",
        "alpha_i = np.random.normal(loc=0, scale=2, size=N)\n",
        "# On cr√©e un vecteur alpha correspondant √† chaque individu\n",
        "alpha = alpha_i[ids]\n",
        "\n",
        "# On ajoute une d√©pendance temporelle (AR(1)) par ex. dans l'erreur\n",
        "# Pour la d√©mo, on va le faire de mani√®re simplifi√©e\n",
        "epsilon = np.zeros(total_obs)\n",
        "rho = 0.5  # auto-corr\n",
        "epsilon[0] = np.random.normal()\n",
        "for t in range(1, total_obs):\n",
        "    # on red√©marre un cycle quand on change d'individu\n",
        "    if ids[t] != ids[t-1]:\n",
        "        epsilon[t] = np.random.normal()\n",
        "    else:\n",
        "        epsilon[t] = rho * epsilon[t-1] + np.random.normal()\n",
        "\n",
        "# On g√©n√®re la variable d√©pendante\n",
        "# Petit mod√®le: Y = 1 + 0.5*X1 + (-0.3)*X2 + alpha_i + epsilon\n",
        "true_beta = [1.0, 0.5, -0.3]\n",
        "Y = true_beta[0] + true_beta[1]*X1 + true_beta[2]*X2 + alpha + epsilon\n",
        "\n",
        "# On met le tout dans un DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'id': ids,\n",
        "    'time': time,\n",
        "    'X1': X1,\n",
        "    'X2': X2,\n",
        "    'Y': Y\n",
        "})\n",
        "\n",
        "# --------------------------------------\n",
        "# 2. Estimation \"na√Øve\" OLS group√©\n",
        "#    (Sans correction d'effets fixes ou corr√©lation)\n",
        "#    Juste pour illustrer le block bootstrap\n",
        "# --------------------------------------\n",
        "# On pr√©pare la matrice de r√©gression\n",
        "X_mat = sm.add_constant(df[['X1','X2']])\n",
        "y_vec = df['Y']\n",
        "\n",
        "model = sm.OLS(y_vec, X_mat).fit()\n",
        "print(\"\\n=== OLS panel na√Øf (pas de FE) ===\")\n",
        "print(model.summary())\n",
        "\n",
        "# --------------------------------------\n",
        "# 3. Block bootstrap sur la dimension temporelle\n",
        "# --------------------------------------\n",
        "def block_bootstrap_panel(df, block_len=2):\n",
        "    \"\"\"\n",
        "    On va resampler des blocs temporels [t, t+1, ..., t+block_len-1]\n",
        "    pour T total = 8 (par ex.).\n",
        "    On fait l'hypoth√®se que tous les individus ont la m√™me suite de temps.\n",
        "\n",
        "    df doit contenir les colonnes: 'id', 'time'\n",
        "    \"\"\"\n",
        "    unique_times = df['time'].unique()\n",
        "    n_blocks = int(np.ceil(len(unique_times) / block_len))\n",
        "\n",
        "    # On va tirer n_blocks blocs (avec remise) parmi les \"blocs\" de temps possibles\n",
        "    # ex. blocks possibles: [0,1], [1,2], [2,3], ... si block_len=2\n",
        "    possible_starts = np.arange(0, len(unique_times) - block_len + 1)\n",
        "    chosen_starts = np.random.choice(possible_starts, size=n_blocks, replace=True)\n",
        "\n",
        "    # on construit la liste des temps s√©lectionn√©s\n",
        "    sampled_times = []\n",
        "    for start in chosen_starts:\n",
        "        block_times = unique_times[start:start+block_len]\n",
        "        sampled_times.extend(block_times)\n",
        "\n",
        "    # Au cas o√π on d√©passe T, on tronque :\n",
        "    sampled_times = sampled_times[:len(unique_times)]\n",
        "\n",
        "    # On r√©cup√®re toutes les lignes correspondantes √† ces temps, pour tous les individus\n",
        "    df_boot = df[df['time'].isin(sampled_times)].copy()\n",
        "\n",
        "    return df_boot\n",
        "\n",
        "# Nombre de r√©plicas\n",
        "n_boot = 300\n",
        "coefs_boot = np.empty((n_boot, 3))  # on suit [const, X1, X2]\n",
        "\n",
        "for i in range(n_boot):\n",
        "    # Tirage de blocs temporels\n",
        "    df_bs = block_bootstrap_panel(df, block_len=2)\n",
        "\n",
        "    # On refait un OLS na√Øf\n",
        "    X_bs = sm.add_constant(df_bs[['X1','X2']])\n",
        "    y_bs = df_bs['Y']\n",
        "    model_bs = sm.OLS(y_bs, X_bs).fit()\n",
        "\n",
        "    coefs_boot[i, :] = model_bs.params.values  # [const, X1, X2]\n",
        "\n",
        "# Intervalle de confiance empirique pour chaque param\n",
        "params_mean = coefs_boot.mean(axis=0)\n",
        "ci_lower = np.percentile(coefs_boot, 2.5, axis=0)\n",
        "ci_upper = np.percentile(coefs_boot, 97.5, axis=0)\n",
        "\n",
        "print(\"\\n=== Block Bootstrap sur le Panel (OLS na√Øf) ===\")\n",
        "for idx, name in enumerate(['const', 'X1', 'X2']):\n",
        "    print(f\"{name} : Moy = {params_mean[idx]:.3f}, IC95% = [{ci_lower[idx]:.3f}, {ci_upper[idx]:.3f}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNHyDEA2WI0x",
        "outputId": "b013e25d-1ef4-439b-8248-8784c8df3cd1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== OLS panel na√Øf (pas de FE) ===\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      Y   R-squared:                       0.168\n",
            "Model:                            OLS   Adj. R-squared:                  0.146\n",
            "Method:                 Least Squares   F-statistic:                     7.757\n",
            "Date:                Tue, 11 Mar 2025   Prob (F-statistic):           0.000853\n",
            "Time:                        17:55:07   Log-Likelihood:                -157.28\n",
            "No. Observations:                  80   AIC:                             320.6\n",
            "Df Residuals:                      77   BIC:                             327.7\n",
            "Df Model:                           2                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.9003      0.197      4.562      0.000       0.507       1.293\n",
            "X1             0.3891      0.172      2.263      0.026       0.047       0.731\n",
            "X2            -0.6534      0.194     -3.360      0.001      -1.041      -0.266\n",
            "==============================================================================\n",
            "Omnibus:                        0.751   Durbin-Watson:                   0.587\n",
            "Prob(Omnibus):                  0.687   Jarque-Bera (JB):                0.820\n",
            "Skew:                          -0.217   Prob(JB):                        0.664\n",
            "Kurtosis:                       2.761   Cond. No.                         1.19\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "\n",
            "=== Block Bootstrap sur le Panel (OLS na√Øf) ===\n",
            "const : Moy = 0.894, IC95% = [0.764, 1.016]\n",
            "X1 : Moy = 0.394, IC95% = [0.178, 0.548]\n",
            "X2 : Moy = -0.675, IC95% = [-0.783, -0.558]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1RzYRf4-WVUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Boostraped o√π il y a de fortes corr√©lations Within √† l'int√©rieur m√™me du groupe entre les variables."
      ],
      "metadata": {
        "id": "Ve0w_vzaWL8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "np.random.seed(999)\n",
        "\n",
        "# -----------------------------------\n",
        "# 1. G√©n√©ration des donn√©es avec groupes\n",
        "# -----------------------------------\n",
        "G = 20         # nombre de groupes\n",
        "mean_size = 30 # taille moyenne par groupe\n",
        "group_list = []\n",
        "X_list = []\n",
        "y_list = []\n",
        "\n",
        "beta_0, beta_1 = 2.0, 1.0  # param√®tres \"vrais\"\n",
        "\n",
        "for g in range(G):\n",
        "    # Taille du groupe = moyenne +/- un peu d'al√©atoire\n",
        "    group_size = np.random.poisson(mean_size) + 10\n",
        "    # Variable X\n",
        "    Xg = np.random.normal(loc=g, scale=1.0, size=group_size)\n",
        "    # Effet fixe (al√©atoire) de groupe\n",
        "    alpha_g = np.random.normal(0, 2)\n",
        "    # Bruit\n",
        "    eps = np.random.normal(0, 1, size=group_size)\n",
        "\n",
        "    yg = beta_0 + alpha_g + beta_1 * Xg + eps\n",
        "\n",
        "    # On stocke\n",
        "    group_list.extend([g]*group_size)\n",
        "    X_list.extend(Xg)\n",
        "    y_list.extend(yg)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'group': group_list,\n",
        "    'X': X_list,\n",
        "    'Y': y_list\n",
        "})\n",
        "\n",
        "# Pr√©pare matrice\n",
        "Xmat = sm.add_constant(df['X'])\n",
        "yvec = df['Y']\n",
        "\n",
        "model_ols_cluster = sm.OLS(yvec, Xmat).fit()\n",
        "print(\"=== OLS classique (pas de correction cluster) ===\")\n",
        "print(model_ols_cluster.summary())\n",
        "\n",
        "# -----------------------------------\n",
        "# 2. Cluster bootstrap\n",
        "# -----------------------------------\n",
        "def cluster_bootstrap(df, group_col='group'):\n",
        "    \"\"\"\n",
        "    Tire des groupes (avec remise),\n",
        "    puis reprend TOUTES les observations de chaque groupe tir√©.\n",
        "    \"\"\"\n",
        "    groups = df[group_col].unique()\n",
        "    n_groups = len(groups)\n",
        "\n",
        "    # tirage avec remise des groupes\n",
        "    chosen_groups = np.random.choice(groups, size=n_groups, replace=True)\n",
        "    # on construit la base bootstrap\n",
        "    df_boot = pd.concat([df[df[group_col] == g] for g in chosen_groups], axis=0)\n",
        "\n",
        "    return df_boot\n",
        "\n",
        "n_boot = 500\n",
        "coef_boot = np.empty(n_boot)\n",
        "\n",
        "for i in range(n_boot):\n",
        "    df_bs = cluster_bootstrap(df, group_col='group')\n",
        "\n",
        "    X_bs = sm.add_constant(df_bs['X'])\n",
        "    y_bs = df_bs['Y']\n",
        "    model_bs = sm.OLS(y_bs, X_bs).fit()\n",
        "\n",
        "    # On stocke le coefficient sur X\n",
        "    coef_boot[i] = model_bs.params['X']\n",
        "\n",
        "# IC empirique sur beta_1\n",
        "beta1_lower = np.percentile(coef_boot, 2.5)\n",
        "beta1_upper = np.percentile(coef_boot, 97.5)\n",
        "print(\"\\n=== Cluster Bootstrap ===\")\n",
        "print(f\"  Moyenne du coeff X    : {coef_boot.mean():.3f}\")\n",
        "print(f\"  IC 95% (cluster)       : [{beta1_lower:.3f}, {beta1_upper:.3f}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30RbV9knWWd9",
        "outputId": "3058a1a0-62fc-43be-ae47-b1ff0e7ee7ed"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== OLS classique (pas de correction cluster) ===\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      Y   R-squared:                       0.905\n",
            "Model:                            OLS   Adj. R-squared:                  0.905\n",
            "Method:                 Least Squares   F-statistic:                     7116.\n",
            "Date:                Tue, 11 Mar 2025   Prob (F-statistic):               0.00\n",
            "Time:                        17:56:03   Log-Likelihood:                -1621.4\n",
            "No. Observations:                 747   AIC:                             3247.\n",
            "Df Residuals:                     745   BIC:                             3256.\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          1.3129      0.146      8.991      0.000       1.026       1.600\n",
            "X              1.1079      0.013     84.354      0.000       1.082       1.134\n",
            "==============================================================================\n",
            "Omnibus:                      124.281   Durbin-Watson:                   0.536\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              219.563\n",
            "Skew:                          -1.010   Prob(JB):                     2.10e-48\n",
            "Kurtosis:                       4.724   Cond. No.                         21.0\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "\n",
            "=== Cluster Bootstrap ===\n",
            "  Moyenne du coeff X    : 1.102\n",
            "  IC 95% (cluster)       : [0.959, 1.264]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "). Dans de nombreux travaux en √©conomie ou sciences sociales, on dispose de donn√©es group√©es‚ÄØ:\n",
        "\n",
        "exemple : des m√©nages appartenant √† des villages (cluster = village),\n",
        "exemple : des travailleurs employ√©s dans une m√™me entreprise (cluster = entreprise),\n",
        "exemple : des √©l√®ves au sein de la m√™me √©cole (cluster = √©cole).\n",
        "Comme les observations √† l‚Äôint√©rieur d‚Äôun m√™me cluster sont corr√©l√©es (m√™me lieu, m√™me environnement socio-√©conomique, m√™me politique locale, etc.), l‚Äôhypoth√®se d‚Äôind√©pendance habituelle entre chaque observation n‚Äôest pas respect√©e. Les estimateurs classiques d‚Äôerreur-standard (et donc d‚Äôintervalle de confiance) peuvent √™tre biais√©s, g√©n√©ralement en sous-estimant la variance."
      ],
      "metadata": {
        "id": "gmx0jgU8Wn06"
      }
    }
  ]
}